{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b585c08-a35e-4241-91c4-036468d9593f",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "# Load RoBERTa model and tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta   = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# 1) Freeze all parameters\n",
    "for param in roberta.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2) Unfreeze last 2 encoder layers\n",
    "for layer in roberta.encoder.layer[-2:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# 3) Unfreeze the pooler if present (gives you a [CLS]–to–sentence vector head)\n",
    "if hasattr(roberta, \"pooler\"):\n",
    "    for param in roberta.pooler.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Encode a sentence via the [CLS] token\n",
    "def encode_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = roberta(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0]  # [CLS] \n",
    "\n",
    "# Quick smoke test\n",
    "text_emb = encode_text(\"Test sentence for fine-tuning.\")\n",
    "\n",
    "print(\"Text embedding shape:\", text_emb.shape)  # → [1, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efa237d-5351-4d8a-bee8-9a184b4b5726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load CLIP model and processor\n",
    "clip_model     = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# 1) Freeze all vision parameters\n",
    "for param in clip_model.vision_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2) Unfreeze last 2 vision encoder layers\n",
    "for layer in clip_model.vision_model.encoder.layers[-2:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# 3) Unfreeze the visual projection head\n",
    "for param in clip_model.visual_projection.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Encode an image (gradients enabled on last layers)\n",
    "def encode_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "    image_emb = clip_model.get_image_features(**inputs)\n",
    "    return image_emb  # [1, 512]\n",
    "\n",
    "# Quick smoke test\n",
    "image_emb = encode_image(\"/Users/kevinpatel/UWF/Donald.jpeg\")\n",
    "print(\"Image embedding shape:\", image_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee4c7bd-fdad-47cf-abe7-0e3226bba9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch.nn as nn\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Extract entities\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\"]]\n",
    "\n",
    "\n",
    "#Test\n",
    "text = \"Apple Inc. was founded by Steve Jobs and is headquartered in Cupertino.\"\n",
    "print(extract_entities(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109a0844-9517-4074-89e1-ceaaf6b8d631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# Step 1: Get QID\n",
    "def get_entity_qid(entity_label):\n",
    "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "    query = f\"\"\"\n",
    "    SELECT ?item WHERE {{\n",
    "      ?item rdfs:label \"{entity_label}\"@en.\n",
    "    }} LIMIT 1\n",
    "    \"\"\"\n",
    "    sparql = SPARQLWrapper(endpoint_url)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    sparql.addCustomHttpHeader(\"User-Agent\", \"KITE/1.0 (kevin@yourdomain.com)\")\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        bindings = results[\"results\"][\"bindings\"]\n",
    "        if bindings:\n",
    "            entity_url = bindings[0][\"item\"][\"value\"]\n",
    "            return entity_url.split(\"/\")[-1]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching QID:\", e)\n",
    "        return None\n",
    "\n",
    "# Step 2: Query facts with QID\n",
    "def get_wikidata_facts_by_qid(qid):\n",
    "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "    query = f\"\"\"\n",
    "    SELECT ?propertyLabel ?valueLabel WHERE {{\n",
    "      wd:{qid} ?prop ?value.\n",
    "      ?property wikibase:directClaim ?prop.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "    }} LIMIT 20\n",
    "    \"\"\"\n",
    "    sparql = SPARQLWrapper(endpoint_url)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    sparql.addCustomHttpHeader(\"User-Agent\", \"KITE/1.0 (kevin@yourdomain.com)\")\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        facts = []\n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            prop = result[\"propertyLabel\"][\"value\"]\n",
    "            val = result[\"valueLabel\"][\"value\"]\n",
    "            facts.append((prop, val))\n",
    "        return facts\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching facts:\", e)\n",
    "        return []\n",
    "\n",
    "print(\"Done!\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b56392-b93f-4931-b58b-8f9d6dcd4616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing usage\n",
    "entity_name = \"Donald Trump\"\n",
    "\n",
    "qid = get_entity_qid(entity_name)\n",
    "if qid:\n",
    "    print(f\"\\nWikidata QID for '{entity_name}': {qid}\")\n",
    "    facts = get_wikidata_facts_by_qid(qid)\n",
    "    print(f\"\\nTop {len(facts)} facts about {entity_name}:\")\n",
    "    for prop, val in facts:\n",
    "        print(f\"  {prop}: {val}\")\n",
    "else:\n",
    "    print(f\"No Wikidata QID found for '{entity_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d98d1d-c27a-4e0e-9579-d6971bd4ed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert facts into a graph: nodes and edges\n",
    "def build_knowledge_graph(entity_name, facts):\n",
    "    nodes = set()\n",
    "    edges = []\n",
    "\n",
    "    # Add the main entity as a node\n",
    "    nodes.add(entity_name)\n",
    "\n",
    "    for prop, val in facts:\n",
    "        nodes.add(val)\n",
    "        edges.append((entity_name, val))  # edge: entity → value\n",
    "\n",
    "    node_list = list(nodes)\n",
    "    node_index = {node: idx for idx, node in enumerate(node_list)}\n",
    "\n",
    "    # Convert edges to index format for PyTorch Geometric\n",
    "    edge_index = torch.tensor([[node_index[src], node_index[dst]] for src, dst in edges], dtype=torch.long).T\n",
    "\n",
    "    return node_list, edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eff22aa-0d61-4934-8148-718789031567",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = \"Barack Obama\"\n",
    "qid = get_entity_qid(entity)\n",
    "facts = get_wikidata_facts_by_qid(qid)\n",
    "\n",
    "node_list, edge_index = build_knowledge_graph(entity, facts)\n",
    "\n",
    "print(\"Nodes:\", node_list)\n",
    "print(\"Edge Index:\\n\", edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43fc0ab-0641-432a-b189-38670a5cb31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_nodes_with_roberta(node_list):\n",
    "    embeddings = []\n",
    "    for node in node_list:\n",
    "        emb = encode_text(node)\n",
    "        embeddings.append(emb)\n",
    "    return torch.vstack(embeddings)  # Shape: [num_nodes, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1b80f4-da22-4803-90ae-bf8efe490137",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = encode_nodes_with_roberta(node_list)\n",
    "print(\"Node Embedding Shape:\", node_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a9f4c-a817-4404-b198-6c4d068a1792",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.1.0+cpu.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3350945-a0e3-434a-a2b5-aa227b5c5447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test to see previous worked\n",
    "from torch_geometric.data import Data\n",
    "print(\"PyG is ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b7077-9569-4910-9d0c-a11f894a521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch-geometric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5412d1cd-062a-48b7-9fe1-5c77ade1e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "#Knowledge Graph GAT",
    "class KnowledgeGraphGAT(nn.Module):\n",
    "    def __init__(self, in_channels: int = 768, out_channels: int = 256):\n",
    "        super(KnowledgeGraphGAT, self).__init__()\n",
    "        self.gat1 = GATConv(in_channels, 256, heads=2, concat=True)\n",
    "        self.gat2 = GATConv(512, out_channels, heads=1, concat=False)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Cross-Modal Transformer Fusion ",
    "class CrossModalTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_dim:  int = 768,\n",
    "        img_dim:   int = 512,\n",
    "        kg_dim:    int = 256,\n",
    "        token_dim: int = 512,\n",
    "        nhead:     int = 8,\n",
    "        layers:    int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Project each modality to the common token_dim\n",
    "        self.proj_t = nn.Linear(text_dim,  token_dim)\n",
    "        self.proj_i = nn.Linear(img_dim,   token_dim)\n",
    "        self.proj_k = nn.Linear(kg_dim,    token_dim)\n",
    "\n",
    "        # Transformer over the 3 tokens\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=token_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=token_dim * 4,\n",
    "            dropout=0.1,\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.transformer = TransformerEncoder(encoder_layer, num_layers=layers)\n",
    "\n",
    "        # Project transformer output back to 1536-dim fusion size\n",
    "        self.out_proj = nn.Linear(token_dim, text_dim + img_dim + kg_dim)\n",
    "\n",
    "    def forward(self, text_emb, image_emb, kg_emb):\n",
    "        # text_emb: [B,768], image_emb: [B,512], kg_emb: [B,256]\n",
    "        t_tok = self.proj_t(text_emb)    # [B, token_dim]\n",
    "        i_tok = self.proj_i(image_emb)   # [B, token_dim]\n",
    "        k_tok = self.proj_k(kg_emb)      # [B, token_dim]\n",
    "\n",
    "        # Stack into [seq_len=3, batch, token_dim] for transformer\n",
    "        tokens = torch.stack([t_tok, i_tok, k_tok], dim=1)  # [B,3,token_dim]\n",
    "        tokens = tokens.permute(1, 0, 2)                    # [3,B,token_dim]\n",
    "        out    = self.transformer(tokens)                   # [3,B,token_dim]\n",
    "        out    = out.permute(1, 0, 2)                       # [B,3,token_dim]\n",
    "\n",
    "        # Mean-pool and project back to fusion dimension\n",
    "        fused_token = out.mean(dim=1)                       # [B, token_dim]\n",
    "        return self.out_proj(fused_token)                   # [B,1536]\n",
    "\n",
    "# Instantiate both modules\n",
    "gat_encoder   = KnowledgeGraphGAT()\n",
    "fusion_xform  = CrossModalTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff2e957-e2ac-45df-aedd-561efd7cb0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fuse knowledge representation with the other two modalities: text and image\n",
    "# Prepare PyG data object\n",
    "graph_data = Data(x=node_features, edge_index=edge_index)\n",
    "\n",
    "\n",
    "# Run the GAT (in eval mode, no gradients needed for now)\n",
    "with torch.no_grad():\n",
    "    knowledge_emb = gat_encoder(graph_data.x, graph_data.edge_index)\n",
    "\n",
    "# Mean-pool the node embeddings to create a single graph-level vector\n",
    "knowledge_pooled = knowledge_emb.mean(dim=0, keepdim=True)  # Shape: [1, 256]\n",
    "\n",
    "print(\"Pooled Knowledge Embedding Shape:\", knowledge_pooled.shape)\n",
    "\n",
    "#knowledge graph has been encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe781b0-a39b-406b-aaa3-4c066bcde47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fusion via Cross-Modal Transformer",
    "fused = fusion_xform(text_emb, image_emb, knowledge_pooled)  # [B, 1536]\n",
    "print(\"Fused Embedding Shape:\", fused.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81876b31-3870-4f63-ad5d-024a70d2555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Tri-modal Classifier with Per-Modality Confidence Heads",
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        # — Classification head (1536 → 512 → 128 → 1) —\n",
    "        self.fc1      = nn.Linear(1536, 512)\n",
    "        self.relu1    = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2      = nn.Linear(512, 128)\n",
    "        self.relu2    = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc3      = nn.Linear(128, 1)   # raw logit output\n",
    "\n",
    "        # — Confidence heads for each modality —\n",
    "        self.text_conf_head  = nn.Linear(768, 1)\n",
    "        self.image_conf_head = nn.Linear(512, 1)\n",
    "        self.kg_conf_head    = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, fused):\n",
    "        # — Main logit prediction —\n",
    "        x = self.relu1(self.fc1(fused))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        logit = self.fc3(x)\n",
    "\n",
    "        # — Slice fused vector back into modalities —\n",
    "        # fused shape: [batch, 1536] = [768 (text) | 512 (image) | 256 (kg)]\n",
    "        text_feat  = fused[:, :768]\n",
    "        image_feat = fused[:, 768:1280]   # 768 + 512 = 1280\n",
    "        kg_feat    = fused[:, 1280:]      # remaining 256 dims\n",
    "\n",
    "        # — Compute per-modality confidences (0–1 range) —\n",
    "        text_conf  = torch.sigmoid(self.text_conf_head(text_feat))\n",
    "        image_conf = torch.sigmoid(self.image_conf_head(image_feat))\n",
    "        kg_conf    = torch.sigmoid(self.kg_conf_head(kg_feat))\n",
    "\n",
    "        return logit, {\n",
    "            \"text_conf\":  text_conf,\n",
    "            \"image_conf\": image_conf,\n",
    "            \"kg_conf\":    kg_conf\n",
    "        }\n",
    "\n",
    "#Smoke-test ",
    "classifier = NewsClassifier()\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    logit, confs = classifier(fused)  # simply pass fused\n",
    "    print(\"Smoke-test logit shape:\", logit.shape)   # should be [1, 1]\n",
    "    print(\"Smoke-test confidences:\", \n",
    "          {k: v.item() for k, v in confs.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe00b01-689d-4d80-a45b-d97f3dc9d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 0) (re-)instantiate your model if needed\n",
    "classifier = NewsClassifier()     # ← make sure this matches your setup\n",
    "classifier.train()\n",
    "\n",
    "# 1) Define a dummy/fresh label\n",
    "label = torch.tensor([[1.0]])     # shape [1,1]; move to GPU if you use one\n",
    "\n",
    "# 2) Make sure `fused` exists:\n",
    "#    if you haven’t just run the fusion cell, recreate it:\n",
    "# fused = torch.cat([text_emb, image_emb, knowledge_pooled], dim=1)\n",
    "\n",
    "# 3) Forward + compute BCEWithLogits loss\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "logits, _ = classifier(fused)\n",
    "# unpack the (logit, confs) tuple\n",
    "loss      = criterion(logits, label)\n",
    "print(f\"Loss value: {loss.item():.4f}\")\n",
    "\n",
    "# 4) Backprop test\n",
    "loss.backward()\n",
    "print(\"✅ Backpropagation successful — model is ready for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f05d6b-a718-4acb-a20a-e8dc928dcec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# In-memory caches for entities and facts\n",
    "qid_cache = {}\n",
    "facts_cache = {}\n",
    "\n",
    "class KITEDataset(Dataset):\n",
    "    def __init__(self, data_rows):\n",
    "        self.data = data_rows\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "\n",
    "        # --- Encode text ---\n",
    "        text_emb = encode_text(entry[\"text\"])  # [1, 768]\n",
    "\n",
    "        # --- Encode image ---\n",
    "        image_emb = encode_image(entry[\"image_path\"])  # [1, 512]\n",
    "\n",
    "        # --- Entity Extraction ---\n",
    "        entities = extract_entities(entry[\"text\"])\n",
    "        qid = None\n",
    "        facts = []\n",
    "\n",
    "        if entities:\n",
    "            # Sanitize entity label\n",
    "            raw_entity = entities[0]\n",
    "            clean_entity = raw_entity.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "            # --- QID lookup with caching ---\n",
    "            if clean_entity in qid_cache:\n",
    "                qid = qid_cache[clean_entity]\n",
    "            else:\n",
    "                try:\n",
    "                    qid = get_entity_qid(clean_entity)\n",
    "                    qid_cache[clean_entity] = qid\n",
    "                except Exception:\n",
    "                    qid = None\n",
    "\n",
    "            # --- Fact lookup with caching ---\n",
    "            if qid:\n",
    "                if qid in facts_cache:\n",
    "                    facts = facts_cache[qid]\n",
    "                else:\n",
    "                    try:\n",
    "                        facts = get_wikidata_facts_by_qid(qid)\n",
    "                        facts_cache[qid] = facts\n",
    "                    except Exception:\n",
    "                        facts = []\n",
    "\n",
    "        # Build Knowledge Graph if facts exist ",
    "        if facts:\n",
    "            try:\n",
    "                node_list, edge_index = build_knowledge_graph(clean_entity, facts)\n",
    "                node_features = encode_nodes_with_roberta(node_list)\n",
    "                graph_data = Data(x=node_features, edge_index=edge_index)\n",
    "                gat_encoder = KnowledgeGraphGAT()\n",
    "                with torch.no_grad():\n",
    "                    knowledge_emb = gat_encoder(graph_data.x, graph_data.edge_index)\n",
    "                knowledge_pooled = knowledge_emb.mean(dim=0, keepdim=True)  # [1, 256]\n",
    "            except Exception:\n",
    "                knowledge_pooled = torch.zeros(1, 256)\n",
    "        else:\n",
    "            knowledge_pooled = torch.zeros(1, 256)\n",
    "\n",
    "        #Fuse Embeddings ",
    "        fused = torch.cat([text_emb, image_emb, knowledge_pooled], dim=1)  # [1, 1536]\n",
    "        label = torch.tensor([entry[\"label\"]], dtype=torch.float32)\n",
    "\n",
    "        return fused.squeeze(0), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea76b95-969f-4496-aeef-3e112586a000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input row for validation\n",
    "sample_data = [\n",
    "    {\n",
    "        \"text\": \"Joe Biden visited Ukraine to show support during the crisis.\",\n",
    "        \"image_path\": \"/Users/kevinpatel/UWF/BidenUkraine.jpeg\",  # Replace with a real path\n",
    "        \"label\": 1  # Real news\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create dataset object\n",
    "test_dataset = KITEDataset(sample_data)\n",
    "\n",
    "# Fetch one sample and check shapes\n",
    "sample_fused, sample_label = test_dataset[0]\n",
    "print(\"Fused shape:\", sample_fused.shape)   # Should be [1536]\n",
    "print(\"Label:\", sample_label.item())        # Should be 1 or 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99884145-5ed7-49c0-84de-b7597231416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set batch size (adjust later for full training)\n",
    "batch_size = 1\n",
    "\n",
    "# Wrap dataset",
    "loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Grab a batch and test\n",
    "for batch_fused, batch_label in loader:\n",
    "    print(\"Batch Fused Shape:\", batch_fused.shape)   # Should be [B, 1536]\n",
    "    print(\"Batch Labels:\", batch_label)              # Should be [B, 1] or [B]\n",
    "    break  # Just show one batch for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c972902b-5cec-46ba-abce-577317056d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Unfreeze additional transformer layers",
    "for layer in roberta.encoder.layer[-4:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "if hasattr(roberta, \"pooler\"):\n",
    "    for param in roberta.pooler.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "for layer in clip_model.vision_model.encoder.layers[-4:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "for param in clip_model.visual_projection.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "#Setup with differential LRs & BCEWithLogitsLoss",
    "epochs     = 1              # keep at 1 for quick validation\n",
    "encoder_lr = 5e-6\n",
    "head_lr    = 3e-4\n",
    "optimizer = AdamW([\n",
    "    {\n",
    "        \"params\": list(roberta.encoder.layer[-4:].parameters()) +\n",
    "                  (list(roberta.pooler.parameters()) if hasattr(roberta, \"pooler\") else []),\n",
    "        \"lr\": encoder_lr, \"weight_decay\": 1e-4\n",
    "    },\n",
    "    {\n",
    "        \"params\": list(clip_model.vision_model.encoder.layers[-4:].parameters()) +\n",
    "                  list(clip_model.visual_projection.parameters()),\n",
    "        \"lr\": encoder_lr, \"weight_decay\": 1e-4\n",
    "    },\n",
    "    {\n",
    "        \"params\": classifier.parameters(), \"lr\": head_lr, \"weight_decay\": 1e-3\n",
    "    }\n",
    "])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# ─── Scheduler Setup (5% warmup) ───\n",
    "total_steps  = epochs * len(loader)\n",
    "warmup_steps = int(0.05 * total_steps)\n",
    "scheduler    = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "#Training Loop with Scheduler & Label Smoothing",
    "smooth = 0.1  # label smoothing factor\n",
    "\n",
    "classifier.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch_fused, batch_label in loader:\n",
    "        optimizer.zero_grad()\n",
    "        # only pass in fused; confidences dict will be empty here\n",
    "        logits, _ = classifier(batch_fused)  \n",
    "        # label smoothing: real→0.9, fake→0.1\n",
    "        smoothed = batch_label * (1 - smooth) + 0.5 * smooth\n",
    "        loss = criterion(logits, smoothed)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07269fa-8ad6-41c8-a64c-52b829b0b3d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c51089-2157-42c9-9644-d96fdd98ae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load real and fake datasets using your full paths\n",
    "real_df = pd.read_csv(\"/Users/kevinpatel/Git/FakeNewsNet/dataset/gossipcop_real.csv\")\n",
    "fake_df = pd.read_csv(\"/Users/kevinpatel/Git/FakeNewsNet/dataset/gossipcop_fake.csv\")\n",
    "\n",
    "# Peek at the structure\n",
    "print(\"Real sample:\")\n",
    "print(real_df.head(2))\n",
    "\n",
    "print(\"\\nFake sample:\")\n",
    "print(fake_df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ed87e2-0a8d-4dc2-a200-79a025ebd1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import fnmatch\n",
    "\n",
    "valid_exts = [\".jpg\", \".jpeg\", \".png\", \".JPG\"]\n",
    "\n",
    "def is_valid_image(path):\n",
    "    try:\n",
    "        Image.open(path).verify()\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def collect_article_samples(root_dir, label):\n",
    "    samples = []\n",
    "    for article_folder in os.listdir(root_dir):\n",
    "        folder_path = os.path.join(root_dir, article_folder)\n",
    "        content_path = os.path.join(folder_path, \"news content.json\")\n",
    "\n",
    "        if not os.path.isfile(content_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(content_path, \"r\") as f:\n",
    "                content = json.load(f)\n",
    "\n",
    "            text = content.get(\"text\") or content.get(\"title\")\n",
    "            if not text or len(text) < 50:\n",
    "                continue\n",
    "\n",
    "            # Prioritize top images first\n",
    "            top_image = None\n",
    "            fallback_image = None\n",
    "\n",
    "            for fname in os.listdir(folder_path):\n",
    "                img_path = os.path.join(folder_path, fname)\n",
    "                if not os.path.isfile(img_path):\n",
    "                    continue\n",
    "\n",
    "                ext_match = any(fname.lower().endswith(ext) for ext in valid_exts)\n",
    "                if not ext_match:\n",
    "                    continue\n",
    "\n",
    "                if fnmatch.fnmatch(fname.lower(), \"top*\") and is_valid_image(img_path):\n",
    "                    top_image = img_path\n",
    "                    break  # Stop if top image is valid\n",
    "                elif is_valid_image(img_path) and fallback_image is None:\n",
    "                    fallback_image = img_path\n",
    "\n",
    "            # Final decision\n",
    "            selected_img = top_image or fallback_image\n",
    "            if selected_img:\n",
    "                samples.append({\n",
    "                    \"text\": text,\n",
    "                    \"image_path\": selected_img,\n",
    "                    \"label\": label\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error reading {content_path}: {e}\")\n",
    "\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f624d2-877b-4db0-9801-b6028a868d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "#Folder paths",
    "base_dir = Path(\"/Users/kevinpatel/Git/FakeNewsNet/code/fakenewsnet_dataset\")\n",
    "real_gossipcop    = base_dir / \"gossipcop/real\"\n",
    "fake_gossipcop    = base_dir / \"gossipcop/fake\"\n",
    "real_politifact   = base_dir / \"politifact/real\"\n",
    "fake_politifact   = base_dir / \"politifact/fake\"\n",
    "\n",
    "#Collect samples ",
    "kite_data = []\n",
    "\n",
    "print(\"🔍 Loading articles from GossipCop (real)...\")\n",
    "start = time()\n",
    "gc_real = collect_article_samples(str(real_gossipcop), label=1)\n",
    "print(f\"  → {len(gc_real)} samples loaded [{time() - start:.2f}s]\")\n",
    "kite_data += gc_real\n",
    "\n",
    "print(\"🔍 Loading articles from GossipCop (fake)...\")\n",
    "start = time()\n",
    "gc_fake = collect_article_samples(str(fake_gossipcop), label=0)\n",
    "print(f\"  → {len(gc_fake)} samples loaded [{time() - start:.2f}s]\")\n",
    "kite_data += gc_fake\n",
    "\n",
    "print(\"🔍 Loading articles from PolitiFact (real)...\")\n",
    "start = time()\n",
    "pf_real = collect_article_samples(str(real_politifact), label=1)\n",
    "print(f\"  → {len(pf_real)} samples loaded [{time() - start:.2f}s]\")\n",
    "kite_data += pf_real\n",
    "\n",
    "print(\"🔍 Loading articles from PolitiFact (fake)...\")\n",
    "start = time()\n",
    "pf_fake = collect_article_samples(str(fake_politifact), label=0)\n",
    "print(f\"  → {len(pf_fake)} samples loaded [{time() - start:.2f}s]\")\n",
    "kite_data += pf_fake\n",
    "\n",
    "print(f\"\\n📦 Total usable samples: {len(kite_data)}\")\n",
    "\n",
    "#Full-data split per source",
    "\n",
    "# GossipCop split\n",
    "gc_samples = gc_real + gc_fake\n",
    "gc_labels  = [row[\"label\"] for row in gc_samples]\n",
    "gc_train, gc_test = train_test_split(\n",
    "    gc_samples,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=gc_labels\n",
    ")\n",
    "print(f\"🔧 GossipCop train/test: {len(gc_train)}/{len(gc_test)} samples\")\n",
    "\n",
    "# PolitiFact split\n",
    "pf_samples = pf_real + pf_fake\n",
    "pf_labels  = [row[\"label\"] for row in pf_samples]\n",
    "pf_train, pf_test = train_test_split(\n",
    "    pf_samples,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=pf_labels\n",
    ")\n",
    "print(f\"🔧 PolitiFact train/test: {len(pf_train)}/{len(pf_test)} samples\")\n",
    "\n",
    "#Build KITE datasets & DataLoaders for each",
    "\n",
    "# GossipCop\n",
    "gc_train_ds = KITEDataset(gc_train)\n",
    "gc_test_ds  = KITEDataset(gc_test)\n",
    "gc_train_loader = DataLoader(gc_train_ds, batch_size=32, shuffle=True)\n",
    "gc_test_loader  = DataLoader(gc_test_ds,  batch_size=32)\n",
    "print(\"🚀 GossipCop DataLoaders ready\")\n",
    "\n",
    "# PolitiFact\n",
    "pf_train_ds = KITEDataset(pf_train)\n",
    "pf_test_ds  = KITEDataset(pf_test)\n",
    "pf_train_loader = DataLoader(pf_train_ds, batch_size=32, shuffle=True)\n",
    "pf_test_loader  = DataLoader(pf_test_ds,  batch_size=32)\n",
    "print(\"🚀 PolitiFact DataLoaders ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528d9e4a-3190-4d8f-808f-cb6942d6fe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Focal Loss Definition",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=0.25, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        p_t = torch.exp(-bce)\n",
    "        loss = self.alpha * (1 - p_t) ** self.gamma * bce\n",
    "        return loss.mean() if self.reduction == 'mean' else loss.sum()\n",
    "\n",
    "#Training + Eval on Both Splits with Focal Loss + Threshold Tuning",
    "epochs, patience = 10, 3\n",
    "results = {}\n",
    "\n",
    "for name, tr_loader, te_loader in [\n",
    "    (\"PolitiFact\", pf_train_loader, pf_test_loader),\n",
    "    (\"GossipCop\",  gc_train_loader, gc_test_loader),\n",
    "]:\n",
    "    print(f\"Running on {name} split")",
    "\n",
    "    # 1) Re-init model, optimizer, scheduler, loss\n",
    "    classifier = NewsClassifier()\n",
    "    optimizer  = AdamW([\n",
    "        {\"params\": roberta.encoder.layer[-4:].parameters(),           \"lr\": 5e-6, \"weight_decay\": 1e-4},\n",
    "        {\"params\": roberta.pooler.parameters(),                        \"lr\": 5e-6, \"weight_decay\": 1e-4},\n",
    "        {\"params\": clip_model.vision_model.encoder.layers[-4:].parameters(), \n",
    "         \"lr\": 5e-6, \"weight_decay\": 1e-4},\n",
    "        {\"params\": clip_model.visual_projection.parameters(),          \"lr\": 5e-6, \"weight_decay\": 1e-4},\n",
    "        {\"params\": classifier.parameters(),                           \"lr\": 3e-4, \"weight_decay\": 1e-3},\n",
    "    ])\n",
    "    criterion = FocalLoss(gamma=2.0, alpha=0.25)\n",
    "    total_steps = epochs * len(tr_loader)\n",
    "    scheduler   = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_val_acc, no_improve = 0.0, 0\n",
    "    best_thr_for_split = 0.5\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\n— Epoch {epoch}/{epochs} —\")\n",
    "        # ——— Training ———\n",
    "        classifier.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (x, y) in enumerate(tr_loader, start=1):\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = classifier(x)                  # unpack tuple here\n",
    "            loss      = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 0 or i == len(tr_loader):\n",
    "                print(f\"  [Train] Batch {i}/{len(tr_loader)} – Loss: {loss:.4f}\")\n",
    "        print(f\"  Avg Train Loss: {running_loss/len(tr_loader):.4f}\")\n",
    "\n",
    "        #Validation & Threshold Sweep",
    "        classifier.eval()\n",
    "        all_probs, all_y = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in te_loader:\n",
    "                logits, _ = classifier(x)             # unpack tuple here\n",
    "                probs      = torch.sigmoid(logits).view(-1).tolist()\n",
    "                labels     = y.view(-1).tolist()\n",
    "                all_probs.extend(probs)\n",
    "                all_y.extend(labels)\n",
    "\n",
    "        # compute precision/recall curve & pick threshold maxing F1\n",
    "        precisions, recalls, ths = precision_recall_curve(all_y, all_probs)\n",
    "        f1_scores = 2 * precisions * recalls / (precisions + recalls + 1e-8)\n",
    "        best_idx  = f1_scores.argmax()\n",
    "        best_thr  = ths[best_idx]\n",
    "        best_thr_for_split = best_thr\n",
    "\n",
    "        # compute early-stop metric (accuracy) at this threshold\n",
    "        val_preds = [1 if p > best_thr else 0 for p in all_probs]\n",
    "        val_acc   = accuracy_score(all_y, val_preds)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc, no_improve = val_acc, 0\n",
    "            torch.save(classifier.state_dict(), f\"best_{name}.pt\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            print(f\"    → No improvement for {no_improve}/{patience} epochs\")\n",
    "            if no_improve >= patience:\n",
    "                print(f\"    Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    #Final Eval + collect confidences",
    "    classifier.load_state_dict(torch.load(f\"best_{name}.pt\"))\n",
    "    classifier.eval()\n",
    "\n",
    "    final_probs, final_y = [], []\n",
    "    text_confs, image_confs, kg_confs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in te_loader:\n",
    "            # 1) get model outputs\n",
    "            logit, confs = classifier(x)\n",
    "\n",
    "            # 2) probabilities and true labels\n",
    "            probs  = torch.sigmoid(logit).view(-1).tolist()\n",
    "            labels = y.view(-1).tolist()\n",
    "\n",
    "            final_probs.extend(probs)\n",
    "            final_y.extend(labels)\n",
    "\n",
    "            # 3) per-modality confidences\n",
    "            text_confs.extend(  confs['text_conf'].view(-1).tolist() )\n",
    "            image_confs.extend( confs['image_conf'].view(-1).tolist() )\n",
    "            kg_confs.extend(    confs['kg_conf'].view(-1).tolist() )\n",
    "\n",
    "    # apply best threshold\n",
    "    final_preds = [1 if p > best_thr_for_split else 0 for p in final_probs]\n",
    "\n",
    "    # compute averages for confidences\n",
    "    avg_txt = sum(text_confs)  / len(text_confs)  if text_confs  else 0.0\n",
    "    avg_img = sum(image_confs) / len(image_confs) if image_confs else 0.0\n",
    "    avg_kg  = sum(kg_confs)    / len(kg_confs)    if kg_confs    else 0.0\n",
    "\n",
    "    # store metrics + averages\n",
    "    results[name] = {\n",
    "        \"accuracy\":       accuracy_score(final_y, final_preds),\n",
    "        \"f1\":             f1_score(final_y, final_preds),\n",
    "        \"precision\":      precision_score(final_y, final_preds),\n",
    "        \"recall\":         recall_score(final_y, final_preds),\n",
    "        \"threshold\":      best_thr_for_split,\n",
    "        \"avg_text_conf\":  avg_txt,\n",
    "        \"avg_image_conf\": avg_img,\n",
    "        \"avg_kg_conf\":    avg_kg,\n",
    "    }\n",
    "    \n",
    "\n",
    "for name, m in results.items():\n",
    "        print(f\"\\n📊 Final {name} Results:\")\n",
    "        print(f\"✅ Accuracy:  {m['accuracy']:.4f}\")\n",
    "        print(f\"🎯 F1 Score:  {m['f1']:.4f}\")\n",
    "        print(f\"🔍 Precision: {m['precision']:.4f}\")\n",
    "        print(f\"📥 Recall:    {m['recall']:.4f}\")\n",
    "        print()\n",
    "        print(f\"📊 Final {name} Results:\")\n",
    "        print(f\"📖 Avg Text Confidence:  {m['avg_text_conf']:.4f}\")\n",
    "        print(f\"🖼️  Avg Image Confidence: {m['avg_image_conf']:.4f}\")\n",
    "        print(f\"📚 Avg KG Confidence:    {m['avg_kg_conf']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
