{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b585c08-a35e-4241-91c4-036468d9593f",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinpatel/venv_310/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embedding shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "# Load RoBERTa model and tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta   = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# 1) Freeze all parameters\n",
    "for param in roberta.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2) Unfreeze last 2 encoder layers\n",
    "for layer in roberta.encoder.layer[-2:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# 3) Unfreeze the pooler if present (gives you a [CLS]â€“toâ€“sentence vector head)\n",
    "if hasattr(roberta, \"pooler\"):\n",
    "    for param in roberta.pooler.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Encode a sentence via the [CLS] token\n",
    "def encode_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = roberta(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0]  # [CLS] \n",
    "\n",
    "# Quick smoke test\n",
    "text_emb = encode_text(\"Test sentence for fine-tuning.\")\n",
    "\n",
    "print(\"Text embedding shape:\", text_emb.shape)  # â†’ [1, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2efa237d-5351-4d8a-bee8-9a184b4b5726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinpatel/venv_310/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image embedding shape: torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load CLIP model and processor\n",
    "clip_model     = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# 1) Freeze all vision parameters\n",
    "for param in clip_model.vision_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2) Unfreeze last 2 vision encoder layers\n",
    "for layer in clip_model.vision_model.encoder.layers[-2:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# 3) Unfreeze the visual projection head\n",
    "for param in clip_model.visual_projection.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Encode an image (gradients enabled on last layers)\n",
    "def encode_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "    image_emb = clip_model.get_image_features(**inputs)\n",
    "    return image_emb  # [1, 512]\n",
    "\n",
    "# Quick smoke test\n",
    "image_emb = encode_image(\"/Users/kevinpatel/UWF/Donald.jpeg\")\n",
    "print(\"Image embedding shape:\", image_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee4c7bd-fdad-47cf-abe7-0e3226bba9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple Inc.', 'Steve Jobs', 'Cupertino']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch.nn as nn\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Extract entities\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\"]]\n",
    "\n",
    "\n",
    "#Test\n",
    "text = \"Apple Inc. was founded by Steve Jobs and is headquartered in Cupertino.\"\n",
    "print(extract_entities(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "109a0844-9517-4074-89e1-ceaaf6b8d631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# Step 1: Get QID\n",
    "def get_entity_qid(entity_label):\n",
    "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "    query = f\"\"\"\n",
    "    SELECT ?item WHERE {{\n",
    "      ?item rdfs:label \"{entity_label}\"@en.\n",
    "    }} LIMIT 1\n",
    "    \"\"\"\n",
    "    sparql = SPARQLWrapper(endpoint_url)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    sparql.addCustomHttpHeader(\"User-Agent\", \"KITE/1.0 (kevin@yourdomain.com)\")\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        bindings = results[\"results\"][\"bindings\"]\n",
    "        if bindings:\n",
    "            entity_url = bindings[0][\"item\"][\"value\"]\n",
    "            return entity_url.split(\"/\")[-1]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching QID:\", e)\n",
    "        return None\n",
    "\n",
    "# Step 2: Query facts with QID\n",
    "def get_wikidata_facts_by_qid(qid):\n",
    "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "    query = f\"\"\"\n",
    "    SELECT ?propertyLabel ?valueLabel WHERE {{\n",
    "      wd:{qid} ?prop ?value.\n",
    "      ?property wikibase:directClaim ?prop.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "    }} LIMIT 20\n",
    "    \"\"\"\n",
    "    sparql = SPARQLWrapper(endpoint_url)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    sparql.addCustomHttpHeader(\"User-Agent\", \"KITE/1.0 (kevin@yourdomain.com)\")\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        facts = []\n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            prop = result[\"propertyLabel\"][\"value\"]\n",
    "            val = result[\"valueLabel\"][\"value\"]\n",
    "            facts.append((prop, val))\n",
    "        return facts\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching facts:\", e)\n",
    "        return []\n",
    "\n",
    "print(\"Done!\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b56392-b93f-4931-b58b-8f9d6dcd4616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wikidata QID for 'Donald Trump': Q22686\n",
      "\n",
      "Top 20 facts about Donald Trump:\n",
      "  member of political party: Republican Party\n",
      "  native language: English\n",
      "  occupation: actor\n",
      "  occupation: writer\n",
      "  occupation: businessperson\n",
      "  occupation: politician\n",
      "  occupation: entrepreneur\n",
      "  occupation: merchant\n",
      "  occupation: chief executive officer\n",
      "  occupation: investor\n",
      "  occupation: television producer\n",
      "  occupation: business magnate\n",
      "  occupation: film producer\n",
      "  occupation: real estate entrepreneur\n",
      "  occupation: game show host\n",
      "  employer: The Trump Organization\n",
      "  signature: http://commons.wikimedia.org/wiki/Special:FilePath/Donald%20Trump%20%28Presidential%20signature%29.svg\n",
      "  movement: conservatism\n",
      "  movement: isolationism\n",
      "  movement: right-wing populism\n"
     ]
    }
   ],
   "source": [
    "# Testing usage\n",
    "entity_name = \"Donald Trump\"\n",
    "\n",
    "qid = get_entity_qid(entity_name)\n",
    "if qid:\n",
    "    print(f\"\\nWikidata QID for '{entity_name}': {qid}\")\n",
    "    facts = get_wikidata_facts_by_qid(qid)\n",
    "    print(f\"\\nTop {len(facts)} facts about {entity_name}:\")\n",
    "    for prop, val in facts:\n",
    "        print(f\"  {prop}: {val}\")\n",
    "else:\n",
    "    print(f\"No Wikidata QID found for '{entity_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4d98d1d-c27a-4e0e-9579-d6971bd4ed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert facts into a graph: nodes and edges\n",
    "def build_knowledge_graph(entity_name, facts):\n",
    "    nodes = set()\n",
    "    edges = []\n",
    "\n",
    "    # Add the main entity as a node\n",
    "    nodes.add(entity_name)\n",
    "\n",
    "    for prop, val in facts:\n",
    "        nodes.add(val)\n",
    "        edges.append((entity_name, val))  # edge: entity â†’ value\n",
    "\n",
    "    node_list = list(nodes)\n",
    "    node_index = {node: idx for idx, node in enumerate(node_list)}\n",
    "\n",
    "    # Convert edges to index format for PyTorch Geometric\n",
    "    edge_index = torch.tensor([[node_index[src], node_index[dst]] for src, dst in edges], dtype=torch.long).T\n",
    "\n",
    "    return node_list, edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eff22aa-0d61-4934-8148-718789031567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: ['Kenya', 'human', 'Stanley Ann Dunham', 'male', 'Columbia University', 'Kapiolani Medical Center for Women and Children', 'Barack Obama', 'Harvard Law School', 'President of the United States', 'Michelle Obama', 'President-elect of the United States', 'Malia Obama', 'member of the State Senate of Illinois', 'Harvard University', 'United States', 'Barack Obama Sr.', 'Sasha Obama', 'http://commons.wikimedia.org/wiki/Special:FilePath/President%20Barack%20Obama.jpg', 'United States senator', 'http://commons.wikimedia.org/wiki/Special:FilePath/2016%20State%20of%20the%20Union%20Address%20%E2%80%93%20Barack%20Obama%20Presidential%20Library.webm', 'family of Barack Obama']\n",
      "Edge Index:\n",
      " tensor([[ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  6],\n",
      "        [19, 17,  5,  3, 15,  2,  9, 14,  0,  1,  8, 10, 18, 12, 11, 16, 20, 13,\n",
      "          4,  7]])\n"
     ]
    }
   ],
   "source": [
    "entity = \"Barack Obama\"\n",
    "qid = get_entity_qid(entity)\n",
    "facts = get_wikidata_facts_by_qid(qid)\n",
    "\n",
    "node_list, edge_index = build_knowledge_graph(entity, facts)\n",
    "\n",
    "print(\"Nodes:\", node_list)\n",
    "print(\"Edge Index:\\n\", edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f43fc0ab-0641-432a-b189-38670a5cb31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_nodes_with_roberta(node_list):\n",
    "    embeddings = []\n",
    "    for node in node_list:\n",
    "        emb = encode_text(node)\n",
    "        embeddings.append(emb)\n",
    "    return torch.vstack(embeddings)  # Shape: [num_nodes, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc1b80f4-da22-4803-90ae-bf8efe490137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Embedding Shape: torch.Size([21, 768])\n"
     ]
    }
   ],
   "source": [
    "node_features = encode_nodes_with_roberta(node_list)\n",
    "print(\"Node Embedding Shape:\", node_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "508a9f4c-a817-4404-b198-6c4d068a1792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-2.1.0+cpu.html\n",
      "Requirement already satisfied: torch-scatter in ./venv_310/lib/python3.10/site-packages (2.1.1)\n",
      "Requirement already satisfied: torch-sparse in ./venv_310/lib/python3.10/site-packages (0.6.18)\n",
      "Requirement already satisfied: torch-cluster in ./venv_310/lib/python3.10/site-packages (1.6.3)\n",
      "Requirement already satisfied: torch-spline-conv in ./venv_310/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: scipy in ./venv_310/lib/python3.10/site-packages (from torch-sparse) (1.15.3)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in ./venv_310/lib/python3.10/site-packages (from scipy->torch-sparse) (1.26.4)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.1.0+cpu.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3350945-a0e3-434a-a2b5-aa227b5c5447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyG is ready!\n"
     ]
    }
   ],
   "source": [
    "#Test to see previous worked\n",
    "from torch_geometric.data import Data\n",
    "print(\"PyG is ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b68b7077-9569-4910-9d0c-a11f894a521b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-geometric in ./venv_310/lib/python3.10/site-packages (2.5.0)\n",
      "Requirement already satisfied: aiohttp in ./venv_310/lib/python3.10/site-packages (from torch-geometric) (3.12.13)\n",
      "Requirement already satisfied: jinja2 in ./venv_310/lib/python3.10/site-packages (from torch-geometric) (3.1.6)\n",
      "Requirement already satisfied: scikit-learn in ./venv_310/lib/python3.10/site-packages (from torch-geometric) (1.7.0)\n",
      "Requirement already satisfied: scipy in ./venv_310/lib/python3.10/site-packages (from torch-geometric) (1.15.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in ./venv_310/lib/python3.10/site-packages (from torch-geometric) (7.0.0)\n",
      "Requirement already satisfied: numpy in ./venv_310/lib/python3.10/site-packages (from torch-geometric) (1.26.4)\n",
      "Requirement already satisfied: pyparsing in ./venv_310/lib/python3.10/site-packages (from torch-geometric) (3.2.3)\n",
      "Requirement already satisfied: requests in ./venv_310/lib/python3.10/site-packages (from torch-geometric) (2.32.4)\n",
      "Requirement already satisfied: fsspec in ./venv_310/lib/python3.10/site-packages (from torch-geometric) (2025.5.1)\n",
      "Requirement already satisfied: tqdm in ./venv_310/lib/python3.10/site-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv_310/lib/python3.10/site-packages (from aiohttp->torch-geometric) (6.6.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv_310/lib/python3.10/site-packages (from aiohttp->torch-geometric) (0.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv_310/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.7.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./venv_310/lib/python3.10/site-packages (from aiohttp->torch-geometric) (2.6.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv_310/lib/python3.10/site-packages (from aiohttp->torch-geometric) (25.3.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./venv_310/lib/python3.10/site-packages (from aiohttp->torch-geometric) (5.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv_310/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv_310/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.20.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv_310/lib/python3.10/site-packages (from jinja2->torch-geometric) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv_310/lib/python3.10/site-packages (from requests->torch-geometric) (3.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv_310/lib/python3.10/site-packages (from requests->torch-geometric) (2025.6.15)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv_310/lib/python3.10/site-packages (from requests->torch-geometric) (2.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv_310/lib/python3.10/site-packages (from requests->torch-geometric) (3.10)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv_310/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv_310/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in ./venv_310/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.14.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch-geometric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5412d1cd-062a-48b7-9fe1-5c77ade1e2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinpatel/venv_310/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "# â€”â€”â€” Knowledge Graph GAT â€”â€”â€”\n",
    "class KnowledgeGraphGAT(nn.Module):\n",
    "    def __init__(self, in_channels: int = 768, out_channels: int = 256):\n",
    "        super(KnowledgeGraphGAT, self).__init__()\n",
    "        self.gat1 = GATConv(in_channels, 256, heads=2, concat=True)\n",
    "        self.gat2 = GATConv(512, out_channels, heads=1, concat=False)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# â€”â€”â€” Cross-Modal Transformer Fusion â€”â€”â€”\n",
    "class CrossModalTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_dim:  int = 768,\n",
    "        img_dim:   int = 512,\n",
    "        kg_dim:    int = 256,\n",
    "        token_dim: int = 512,\n",
    "        nhead:     int = 8,\n",
    "        layers:    int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Project each modality to the common token_dim\n",
    "        self.proj_t = nn.Linear(text_dim,  token_dim)\n",
    "        self.proj_i = nn.Linear(img_dim,   token_dim)\n",
    "        self.proj_k = nn.Linear(kg_dim,    token_dim)\n",
    "\n",
    "        # Transformer over the 3 tokens\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=token_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=token_dim * 4,\n",
    "            dropout=0.1,\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.transformer = TransformerEncoder(encoder_layer, num_layers=layers)\n",
    "\n",
    "        # Project transformer output back to 1536-dim fusion size\n",
    "        self.out_proj = nn.Linear(token_dim, text_dim + img_dim + kg_dim)\n",
    "\n",
    "    def forward(self, text_emb, image_emb, kg_emb):\n",
    "        # text_emb: [B,768], image_emb: [B,512], kg_emb: [B,256]\n",
    "        t_tok = self.proj_t(text_emb)    # [B, token_dim]\n",
    "        i_tok = self.proj_i(image_emb)   # [B, token_dim]\n",
    "        k_tok = self.proj_k(kg_emb)      # [B, token_dim]\n",
    "\n",
    "        # Stack into [seq_len=3, batch, token_dim] for transformer\n",
    "        tokens = torch.stack([t_tok, i_tok, k_tok], dim=1)  # [B,3,token_dim]\n",
    "        tokens = tokens.permute(1, 0, 2)                    # [3,B,token_dim]\n",
    "        out    = self.transformer(tokens)                   # [3,B,token_dim]\n",
    "        out    = out.permute(1, 0, 2)                       # [B,3,token_dim]\n",
    "\n",
    "        # Mean-pool and project back to fusion dimension\n",
    "        fused_token = out.mean(dim=1)                       # [B, token_dim]\n",
    "        return self.out_proj(fused_token)                   # [B,1536]\n",
    "\n",
    "# Instantiate both modules\n",
    "gat_encoder   = KnowledgeGraphGAT()\n",
    "fusion_xform  = CrossModalTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ff2e957-e2ac-45df-aedd-561efd7cb0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled Knowledge Embedding Shape: torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "#fuse knowledge representation with the other two modalities: text and image\n",
    "# Prepare PyG data object\n",
    "graph_data = Data(x=node_features, edge_index=edge_index)\n",
    "\n",
    "\n",
    "# Run the GAT (in eval mode, no gradients needed for now)\n",
    "with torch.no_grad():\n",
    "    knowledge_emb = gat_encoder(graph_data.x, graph_data.edge_index)\n",
    "\n",
    "# Mean-pool the node embeddings to create a single graph-level vector\n",
    "knowledge_pooled = knowledge_emb.mean(dim=0, keepdim=True)  # Shape: [1, 256]\n",
    "\n",
    "print(\"Pooled Knowledge Embedding Shape:\", knowledge_pooled.shape)\n",
    "\n",
    "#knowledge graph has been encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbe781b0-a39b-406b-aaa3-4c066bcde47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused Embedding Shape: torch.Size([1, 1536])\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€ Fusion via Cross-Modal Transformer â”€â”€â”€\n",
    "fused = fusion_xform(text_emb, image_emb, knowledge_pooled)  # [B, 1536]\n",
    "print(\"Fused Embedding Shape:\", fused.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81876b31-3870-4f63-ad5d-024a70d2555a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoke-test logit shape: torch.Size([1, 1])\n",
      "Smoke-test confidences: {'text_conf': 0.46819454431533813, 'image_conf': 0.5450189709663391, 'kg_conf': 0.40811699628829956}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# â”€â”€â”€ Tri-modal Classifier with Per-Modality Confidence Heads â”€â”€â”€\n",
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        # â€” Classification head (1536 â†’ 512 â†’ 128 â†’ 1) â€”\n",
    "        self.fc1      = nn.Linear(1536, 512)\n",
    "        self.relu1    = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2      = nn.Linear(512, 128)\n",
    "        self.relu2    = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc3      = nn.Linear(128, 1)   # raw logit output\n",
    "\n",
    "        # â€” Confidence heads for each modality â€”\n",
    "        self.text_conf_head  = nn.Linear(768, 1)\n",
    "        self.image_conf_head = nn.Linear(512, 1)\n",
    "        self.kg_conf_head    = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, fused):\n",
    "        # â€” Main logit prediction â€”\n",
    "        x = self.relu1(self.fc1(fused))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        logit = self.fc3(x)\n",
    "\n",
    "        # â€” Slice fused vector back into modalities â€”\n",
    "        # fused shape: [batch, 1536] = [768 (text) | 512 (image) | 256 (kg)]\n",
    "        text_feat  = fused[:, :768]\n",
    "        image_feat = fused[:, 768:1280]   # 768 + 512 = 1280\n",
    "        kg_feat    = fused[:, 1280:]      # remaining 256 dims\n",
    "\n",
    "        # â€” Compute per-modality confidences (0â€“1 range) â€”\n",
    "        text_conf  = torch.sigmoid(self.text_conf_head(text_feat))\n",
    "        image_conf = torch.sigmoid(self.image_conf_head(image_feat))\n",
    "        kg_conf    = torch.sigmoid(self.kg_conf_head(kg_feat))\n",
    "\n",
    "        return logit, {\n",
    "            \"text_conf\":  text_conf,\n",
    "            \"image_conf\": image_conf,\n",
    "            \"kg_conf\":    kg_conf\n",
    "        }\n",
    "\n",
    "# â”€â”€â”€ Smoke-test â”€â”€â”€\n",
    "classifier = NewsClassifier()\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    logit, confs = classifier(fused)  # simply pass fused\n",
    "    print(\"Smoke-test logit shape:\", logit.shape)   # should be [1, 1]\n",
    "    print(\"Smoke-test confidences:\", \n",
    "          {k: v.item() for k, v in confs.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fe00b01-689d-4d80-a45b-d97f3dc9d4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss value: 0.7346\n",
      "âœ… Backpropagation successful â€” model is ready for training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 0) (re-)instantiate your model if needed\n",
    "classifier = NewsClassifier()     # â† make sure this matches your setup\n",
    "classifier.train()\n",
    "\n",
    "# 1) Define a dummy/fresh label\n",
    "label = torch.tensor([[1.0]])     # shape [1,1]; move to GPU if you use one\n",
    "\n",
    "# 2) Make sure `fused` exists:\n",
    "#    if you havenâ€™t just run the fusion cell, recreate it:\n",
    "# fused = torch.cat([text_emb, image_emb, knowledge_pooled], dim=1)\n",
    "\n",
    "# 3) Forward + compute BCEWithLogits loss\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "logits, _ = classifier(fused)\n",
    "# unpack the (logit, confs) tuple\n",
    "loss      = criterion(logits, label)\n",
    "print(f\"Loss value: {loss.item():.4f}\")\n",
    "\n",
    "# 4) Backprop test\n",
    "loss.backward()\n",
    "print(\"âœ… Backpropagation successful â€” model is ready for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5f05d6b-a718-4acb-a20a-e8dc928dcec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# In-memory caches for entities and facts\n",
    "qid_cache = {}\n",
    "facts_cache = {}\n",
    "\n",
    "class KITEDataset(Dataset):\n",
    "    def __init__(self, data_rows):\n",
    "        self.data = data_rows\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "\n",
    "        # --- Encode text ---\n",
    "        text_emb = encode_text(entry[\"text\"])  # [1, 768]\n",
    "\n",
    "        # --- Encode image ---\n",
    "        image_emb = encode_image(entry[\"image_path\"])  # [1, 512]\n",
    "\n",
    "        # --- Entity Extraction ---\n",
    "        entities = extract_entities(entry[\"text\"])\n",
    "        qid = None\n",
    "        facts = []\n",
    "\n",
    "        if entities:\n",
    "            # Sanitize entity label\n",
    "            raw_entity = entities[0]\n",
    "            clean_entity = raw_entity.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "            # --- QID lookup with caching ---\n",
    "            if clean_entity in qid_cache:\n",
    "                qid = qid_cache[clean_entity]\n",
    "            else:\n",
    "                try:\n",
    "                    qid = get_entity_qid(clean_entity)\n",
    "                    qid_cache[clean_entity] = qid\n",
    "                except Exception:\n",
    "                    qid = None\n",
    "\n",
    "            # --- Fact lookup with caching ---\n",
    "            if qid:\n",
    "                if qid in facts_cache:\n",
    "                    facts = facts_cache[qid]\n",
    "                else:\n",
    "                    try:\n",
    "                        facts = get_wikidata_facts_by_qid(qid)\n",
    "                        facts_cache[qid] = facts\n",
    "                    except Exception:\n",
    "                        facts = []\n",
    "\n",
    "        # --- Build Knowledge Graph (if facts exist) ---\n",
    "        if facts:\n",
    "            try:\n",
    "                node_list, edge_index = build_knowledge_graph(clean_entity, facts)\n",
    "                node_features = encode_nodes_with_roberta(node_list)\n",
    "                graph_data = Data(x=node_features, edge_index=edge_index)\n",
    "                gat_encoder = KnowledgeGraphGAT()\n",
    "                with torch.no_grad():\n",
    "                    knowledge_emb = gat_encoder(graph_data.x, graph_data.edge_index)\n",
    "                knowledge_pooled = knowledge_emb.mean(dim=0, keepdim=True)  # [1, 256]\n",
    "            except Exception:\n",
    "                knowledge_pooled = torch.zeros(1, 256)\n",
    "        else:\n",
    "            knowledge_pooled = torch.zeros(1, 256)\n",
    "\n",
    "        # --- Fuse Embeddings ---\n",
    "        fused = torch.cat([text_emb, image_emb, knowledge_pooled], dim=1)  # [1, 1536]\n",
    "        label = torch.tensor([entry[\"label\"]], dtype=torch.float32)\n",
    "\n",
    "        return fused.squeeze(0), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aea76b95-969f-4496-aeef-3e112586a000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused shape: torch.Size([1536])\n",
      "Label: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Example input row for validation\n",
    "sample_data = [\n",
    "    {\n",
    "        \"text\": \"Joe Biden visited Ukraine to show support during the crisis.\",\n",
    "        \"image_path\": \"/Users/kevinpatel/UWF/BidenUkraine.jpeg\",  # Replace with a real path\n",
    "        \"label\": 1  # Real news\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create dataset object\n",
    "test_dataset = KITEDataset(sample_data)\n",
    "\n",
    "# Fetch one sample and check shapes\n",
    "sample_fused, sample_label = test_dataset[0]\n",
    "print(\"Fused shape:\", sample_fused.shape)   # Should be [1536]\n",
    "print(\"Label:\", sample_label.item())        # Should be 1 or 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99884145-5ed7-49c0-84de-b7597231416c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Fused Shape: torch.Size([1, 1536])\n",
      "Batch Labels: tensor([[1.]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set batch size (adjust later for full training)\n",
    "batch_size = 1\n",
    "\n",
    "# Wrap your dataset\n",
    "loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Grab a batch and test\n",
    "for batch_fused, batch_label in loader:\n",
    "    print(\"Batch Fused Shape:\", batch_fused.shape)   # Should be [B, 1536]\n",
    "    print(\"Batch Labels:\", batch_label)              # Should be [B, 1] or [B]\n",
    "    break  # Just show one batch for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c972902b-5cec-46ba-abce-577317056d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 0.7834\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# â”€â”€â”€ Unfreeze additional transformer layers (as before) â”€â”€â”€\n",
    "for layer in roberta.encoder.layer[-4:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "if hasattr(roberta, \"pooler\"):\n",
    "    for param in roberta.pooler.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "for layer in clip_model.vision_model.encoder.layers[-4:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "for param in clip_model.visual_projection.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# â”€â”€â”€ Setup with differential LRs & BCEWithLogitsLoss â”€â”€â”€\n",
    "epochs     = 1              # keep at 1 for quick validation\n",
    "encoder_lr = 5e-6\n",
    "head_lr    = 3e-4\n",
    "optimizer = AdamW([\n",
    "    {\n",
    "        \"params\": list(roberta.encoder.layer[-4:].parameters()) +\n",
    "                  (list(roberta.pooler.parameters()) if hasattr(roberta, \"pooler\") else []),\n",
    "        \"lr\": encoder_lr, \"weight_decay\": 1e-4\n",
    "    },\n",
    "    {\n",
    "        \"params\": list(clip_model.vision_model.encoder.layers[-4:].parameters()) +\n",
    "                  list(clip_model.visual_projection.parameters()),\n",
    "        \"lr\": encoder_lr, \"weight_decay\": 1e-4\n",
    "    },\n",
    "    {\n",
    "        \"params\": classifier.parameters(), \"lr\": head_lr, \"weight_decay\": 1e-3\n",
    "    }\n",
    "])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# â”€â”€â”€ Scheduler Setup (5% warmup) â”€â”€â”€\n",
    "total_steps  = epochs * len(loader)\n",
    "warmup_steps = int(0.05 * total_steps)\n",
    "scheduler    = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "# â”€â”€â”€ Training Loop with Scheduler & Label Smoothing â”€â”€â”€\n",
    "smooth = 0.1  # label smoothing factor\n",
    "\n",
    "classifier.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch_fused, batch_label in loader:\n",
    "        optimizer.zero_grad()\n",
    "        # only pass in fused; confidences dict will be empty here\n",
    "        logits, _ = classifier(batch_fused)  \n",
    "        # label smoothing: realâ†’0.9, fakeâ†’0.1\n",
    "        smoothed = batch_label * (1 - smooth) + 0.5 * smooth\n",
    "        loss = criterion(logits, smoothed)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e07269fa-8ad6-41c8-a64c-52b829b0b3d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./venv_310/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv_310/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./venv_310/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv_310/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv_310/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv_310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76c51089-2157-42c9-9644-d96fdd98ae71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real sample:\n",
      "                 id                                           news_url  \\\n",
      "0  gossipcop-882573  https://www.brides.com/story/teen-mom-jenelle-...   \n",
      "1  gossipcop-875924  https://www.dailymail.co.uk/tvshowbiz/article-...   \n",
      "\n",
      "                                               title  \\\n",
      "0  Teen Mom Star Jenelle Evans' Wedding Dress Is ...   \n",
      "1  Kylie Jenner refusing to discuss Tyga on Life ...   \n",
      "\n",
      "                                           tweet_ids  \n",
      "0  912371411146149888\\t912371528343408641\\t912372...  \n",
      "1  901989917546426369\\t901989992074969089\\t901990...  \n",
      "\n",
      "Fake sample:\n",
      "                     id                                           news_url  \\\n",
      "0  gossipcop-2493749932  www.dailymail.co.uk/tvshowbiz/article-5874213/...   \n",
      "1  gossipcop-4580247171  hollywoodlife.com/2018/05/05/paris-jackson-car...   \n",
      "\n",
      "                                               title  \\\n",
      "0  Did Miley Cyrus and Liam Hemsworth secretly ge...   \n",
      "1  Paris Jackson & Cara Delevingne Enjoy Night Ou...   \n",
      "\n",
      "                                           tweet_ids  \n",
      "0  284329075902926848\\t284332744559968256\\t284335...  \n",
      "1  992895508267130880\\t992897935418503169\\t992899...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load real and fake datasets using your full paths\n",
    "real_df = pd.read_csv(\"/Users/kevinpatel/Git/FakeNewsNet/dataset/gossipcop_real.csv\")\n",
    "fake_df = pd.read_csv(\"/Users/kevinpatel/Git/FakeNewsNet/dataset/gossipcop_fake.csv\")\n",
    "\n",
    "# Peek at the structure\n",
    "print(\"Real sample:\")\n",
    "print(real_df.head(2))\n",
    "\n",
    "print(\"\\nFake sample:\")\n",
    "print(fake_df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2ed87e2-0a8d-4dc2-a200-79a025ebd1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import fnmatch\n",
    "\n",
    "valid_exts = [\".jpg\", \".jpeg\", \".png\", \".JPG\"]\n",
    "\n",
    "def is_valid_image(path):\n",
    "    try:\n",
    "        Image.open(path).verify()\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def collect_article_samples(root_dir, label):\n",
    "    samples = []\n",
    "    for article_folder in os.listdir(root_dir):\n",
    "        folder_path = os.path.join(root_dir, article_folder)\n",
    "        content_path = os.path.join(folder_path, \"news content.json\")\n",
    "\n",
    "        if not os.path.isfile(content_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(content_path, \"r\") as f:\n",
    "                content = json.load(f)\n",
    "\n",
    "            text = content.get(\"text\") or content.get(\"title\")\n",
    "            if not text or len(text) < 50:\n",
    "                continue\n",
    "\n",
    "            # ðŸ–¼ï¸ Prioritize top images first\n",
    "            top_image = None\n",
    "            fallback_image = None\n",
    "\n",
    "            for fname in os.listdir(folder_path):\n",
    "                img_path = os.path.join(folder_path, fname)\n",
    "                if not os.path.isfile(img_path):\n",
    "                    continue\n",
    "\n",
    "                ext_match = any(fname.lower().endswith(ext) for ext in valid_exts)\n",
    "                if not ext_match:\n",
    "                    continue\n",
    "\n",
    "                if fnmatch.fnmatch(fname.lower(), \"top*\") and is_valid_image(img_path):\n",
    "                    top_image = img_path\n",
    "                    break  # Stop if top image is valid\n",
    "                elif is_valid_image(img_path) and fallback_image is None:\n",
    "                    fallback_image = img_path\n",
    "\n",
    "            # Final decision\n",
    "            selected_img = top_image or fallback_image\n",
    "            if selected_img:\n",
    "                samples.append({\n",
    "                    \"text\": text,\n",
    "                    \"image_path\": selected_img,\n",
    "                    \"label\": label\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error reading {content_path}: {e}\")\n",
    "\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65f624d2-877b-4db0-9801-b6028a868d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Loading articles from GossipCop (real)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinpatel/venv_310/lib/python3.10/site-packages/PIL/Image.py:3157: DecompressionBombWarning: Image size (102667500 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ 4610 samples loaded [11.03s]\n",
      "ðŸ” Loading articles from GossipCop (fake)...\n",
      "  â†’ 3838 samples loaded [7.51s]\n",
      "ðŸ” Loading articles from PolitiFact (real)...\n",
      "  â†’ 169 samples loaded [0.19s]\n",
      "ðŸ” Loading articles from PolitiFact (fake)...\n",
      "  â†’ 156 samples loaded [0.33s]\n",
      "\n",
      "ðŸ“¦ Total usable samples: 8773\n",
      "ðŸ”§ GossipCop train/test: 6758/1690 samples\n",
      "ðŸ”§ PolitiFact train/test: 260/65 samples\n",
      "ðŸš€ GossipCop DataLoaders ready\n",
      "ðŸš€ PolitiFact DataLoaders ready\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "# --- Folder paths ---\n",
    "base_dir = Path(\"/Users/kevinpatel/Git/FakeNewsNet/code/fakenewsnet_dataset\")\n",
    "real_gossipcop    = base_dir / \"gossipcop/real\"\n",
    "fake_gossipcop    = base_dir / \"gossipcop/fake\"\n",
    "real_politifact   = base_dir / \"politifact/real\"\n",
    "fake_politifact   = base_dir / \"politifact/fake\"\n",
    "\n",
    "# --- Collect samples ---\n",
    "kite_data = []\n",
    "\n",
    "print(\"ðŸ” Loading articles from GossipCop (real)...\")\n",
    "start = time()\n",
    "gc_real = collect_article_samples(str(real_gossipcop), label=1)\n",
    "print(f\"  â†’ {len(gc_real)} samples loaded [{time() - start:.2f}s]\")\n",
    "kite_data += gc_real\n",
    "\n",
    "print(\"ðŸ” Loading articles from GossipCop (fake)...\")\n",
    "start = time()\n",
    "gc_fake = collect_article_samples(str(fake_gossipcop), label=0)\n",
    "print(f\"  â†’ {len(gc_fake)} samples loaded [{time() - start:.2f}s]\")\n",
    "kite_data += gc_fake\n",
    "\n",
    "print(\"ðŸ” Loading articles from PolitiFact (real)...\")\n",
    "start = time()\n",
    "pf_real = collect_article_samples(str(real_politifact), label=1)\n",
    "print(f\"  â†’ {len(pf_real)} samples loaded [{time() - start:.2f}s]\")\n",
    "kite_data += pf_real\n",
    "\n",
    "print(\"ðŸ” Loading articles from PolitiFact (fake)...\")\n",
    "start = time()\n",
    "pf_fake = collect_article_samples(str(fake_politifact), label=0)\n",
    "print(f\"  â†’ {len(pf_fake)} samples loaded [{time() - start:.2f}s]\")\n",
    "kite_data += pf_fake\n",
    "\n",
    "print(f\"\\nðŸ“¦ Total usable samples: {len(kite_data)}\")\n",
    "\n",
    "# --- Full-data split per source ---\n",
    "\n",
    "# GossipCop split\n",
    "gc_samples = gc_real + gc_fake\n",
    "gc_labels  = [row[\"label\"] for row in gc_samples]\n",
    "gc_train, gc_test = train_test_split(\n",
    "    gc_samples,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=gc_labels\n",
    ")\n",
    "print(f\"ðŸ”§ GossipCop train/test: {len(gc_train)}/{len(gc_test)} samples\")\n",
    "\n",
    "# PolitiFact split\n",
    "pf_samples = pf_real + pf_fake\n",
    "pf_labels  = [row[\"label\"] for row in pf_samples]\n",
    "pf_train, pf_test = train_test_split(\n",
    "    pf_samples,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=pf_labels\n",
    ")\n",
    "print(f\"ðŸ”§ PolitiFact train/test: {len(pf_train)}/{len(pf_test)} samples\")\n",
    "\n",
    "# --- Build KITE datasets & DataLoaders for each ---\n",
    "\n",
    "# GossipCop\n",
    "gc_train_ds = KITEDataset(gc_train)\n",
    "gc_test_ds  = KITEDataset(gc_test)\n",
    "gc_train_loader = DataLoader(gc_train_ds, batch_size=32, shuffle=True)\n",
    "gc_test_loader  = DataLoader(gc_test_ds,  batch_size=32)\n",
    "print(\"ðŸš€ GossipCop DataLoaders ready\")\n",
    "\n",
    "# PolitiFact\n",
    "pf_train_ds = KITEDataset(pf_train)\n",
    "pf_test_ds  = KITEDataset(pf_test)\n",
    "pf_train_loader = DataLoader(pf_train_ds, batch_size=32, shuffle=True)\n",
    "pf_test_loader  = DataLoader(pf_test_ds,  batch_size=32)\n",
    "print(\"ðŸš€ PolitiFact DataLoaders ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "528d9e4a-3190-4d8f-808f-cb6942d6fe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running on PolitiFact split ===\n",
      "\n",
      "â€” Epoch 1/10 â€”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinpatel/venv_310/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/Users/kevinpatel/venv_310/lib/python3.10/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Train] Batch 9/9 â€“ Loss: 0.0408\n",
      "  Avg Train Loss: 0.0430\n",
      "\n",
      "â€” Epoch 2/10 â€”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinpatel/venv_310/lib/python3.10/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Train] Batch 9/9 â€“ Loss: 0.0287\n",
      "  Avg Train Loss: 0.0386\n",
      "\n",
      "â€” Epoch 3/10 â€”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinpatel/venv_310/lib/python3.10/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Train] Batch 9/9 â€“ Loss: 0.0326\n",
      "  Avg Train Loss: 0.0341\n",
      "\n",
      "â€” Epoch 4/10 â€”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinpatel/venv_310/lib/python3.10/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Train] Batch 9/9 â€“ Loss: 0.0177\n",
      "  Avg Train Loss: 0.0258\n",
      "    â†’ No improvement for 1/3 epochs\n",
      "\n",
      "â€” Epoch 5/10 â€”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinpatel/venv_310/lib/python3.10/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Train] Batch 9/9 â€“ Loss: 0.0139\n",
      "  Avg Train Loss: 0.0187\n",
      "    â†’ No improvement for 2/3 epochs\n",
      "\n",
      "â€” Epoch 6/10 â€”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinpatel/venv_310/lib/python3.10/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Train] Batch 9/9 â€“ Loss: 0.0061\n",
      "  Avg Train Loss: 0.0122\n",
      "    â†’ No improvement for 3/3 epochs\n",
      "    Early stopping at epoch 6\n",
      "\n",
      "=== Running on GossipCop split ===\n",
      "\n",
      "â€” Epoch 1/10 â€”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinpatel/venv_310/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/Users/kevinpatel/venv_310/lib/python3.10/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching QID: QueryBadFormed: A bad request has been sent to the endpoint: probably the SPARQL query is badly formed. \n",
      "\n",
      "Response:\n",
      "b'SPARQL-QUERY: queryStr=\\n    SELECT ?item WHERE {\\n      ?item rdfs:label \"Milena Markovna \"Mila\"@en.\\n    } LIMIT 1\\n    \\njava.util.concurrent.ExecutionException: org.openrdf.query.MalformedQueryException: Lexical error at line 3, column 46.  Encountered: \"\\\\\"\" (34), after : \"Mila\"\\n\\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:206)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doGet(QueryServlet.java:290)\\n\\tat com.bigdata.rdf.sail.webapp.RESTServlet.doGet(RESTServlet.java:240)\\n\\tat com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doGet(MultiTenancyServlet.java:273)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFilter.doFilter(ThrottlingFilter.java:322)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.SystemOverloadFilter.doFilter(SystemOverloadFilter.java:84)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat ch.qos.logback.classic.helpers.MDCInsertingServletFilter.doFilter(MDCInsertingServletFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.QueryEventSenderFilter.doFilter(QueryEventSenderFilter.java:125)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.ClientIPFilter.doFilter(ClientIPFilter.java:43)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.JWTIdentityFilter.doFilter(JWTIdentityFilter.java:66)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RealAgentFilter.doFilter(RealAgentFilter.java:33)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RequestConcurrencyFilter.doFilter(RequestConcurrencyFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\\n\\tat java.lang.Thread.run(Thread.java:750)\\nCaused by: org.openrdf.query.MalformedQueryException: Lexical error at line 3, column 46.  Encountered: \"\\\\\"\" (34), after : \"Mila\"\\n\\tat com.bigdata.rdf.sail.sparql.Bigdata2ASTSPARQLParser.parseQuery2(Bigdata2ASTSPARQLParser.java:404)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:741)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:695)\\n\\tat com.bigdata.rdf.task.ApiTaskForIndexManager.call(ApiTaskForIndexManager.java:68)\\n\\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\t... 1 more\\nCaused by: com.bigdata.rdf.sail.sparql.ast.TokenMgrError: Lexical error at line 3, column 46.  Encountered: \"\\\\\"\" (34), after : \"Mila\"\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilderTokenManager.getNextToken(SyntaxTreeBuilderTokenManager.java:3994)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.jj_ntk(SyntaxTreeBuilder.java:9637)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.RDFLiteral(SyntaxTreeBuilder.java:7189)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphTerm(SyntaxTreeBuilder.java:3893)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.VarOrTermOrTRefP(SyntaxTreeBuilder.java:8714)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphNodePath(SyntaxTreeBuilder.java:3786)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.ObjectPath(SyntaxTreeBuilder.java:3467)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.ObjectListPath(SyntaxTreeBuilder.java:3044)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.PropertyListPath(SyntaxTreeBuilder.java:2992)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.TriplesSameSubjectPath(SyntaxTreeBuilder.java:2919)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.TriplesBlock(SyntaxTreeBuilder.java:2312)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.BasicGraphPattern(SyntaxTreeBuilder.java:2097)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphPattern(SyntaxTreeBuilder.java:2034)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GroupGraphPattern(SyntaxTreeBuilder.java:1969)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.WhereClause(SyntaxTreeBuilder.java:1013)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.SelectQuery(SyntaxTreeBuilder.java:377)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.Query(SyntaxTreeBuilder.java:328)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.QueryContainer(SyntaxTreeBuilder.java:216)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.parseQuery(SyntaxTreeBuilder.java:32)\\n\\tat com.bigdata.rdf.sail.sparql.Bigdata2ASTSPARQLParser.parseQuery2(Bigdata2ASTSPARQLParser.java:336)\\n\\t... 7 more\\n'\n",
      "  [Train] Batch 100/212 â€“ Loss: 0.0394\n",
      "Error fetching QID: QueryBadFormed: A bad request has been sent to the endpoint: probably the SPARQL query is badly formed. \n",
      "\n",
      "Response:\n",
      "b'SPARQL-QUERY: queryStr=\\n    SELECT ?item WHERE {\\n      ?item rdfs:label \"Nicole \"Snooki\" Polizzi\"@en.\\n    } LIMIT 1\\n    \\njava.util.concurrent.ExecutionException: org.openrdf.query.MalformedQueryException: Lexical error at line 3, column 39.  Encountered: \"\\\\\"\" (34), after : \"Snooki\"\\n\\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:206)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doGet(QueryServlet.java:290)\\n\\tat com.bigdata.rdf.sail.webapp.RESTServlet.doGet(RESTServlet.java:240)\\n\\tat com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doGet(MultiTenancyServlet.java:273)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFilter.doFilter(ThrottlingFilter.java:322)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.SystemOverloadFilter.doFilter(SystemOverloadFilter.java:84)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat ch.qos.logback.classic.helpers.MDCInsertingServletFilter.doFilter(MDCInsertingServletFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.QueryEventSenderFilter.doFilter(QueryEventSenderFilter.java:125)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.ClientIPFilter.doFilter(ClientIPFilter.java:43)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.JWTIdentityFilter.doFilter(JWTIdentityFilter.java:66)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RealAgentFilter.doFilter(RealAgentFilter.java:33)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RequestConcurrencyFilter.doFilter(RequestConcurrencyFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\\n\\tat java.lang.Thread.run(Thread.java:750)\\nCaused by: org.openrdf.query.MalformedQueryException: Lexical error at line 3, column 39.  Encountered: \"\\\\\"\" (34), after : \"Snooki\"\\n\\tat com.bigdata.rdf.sail.sparql.Bigdata2ASTSPARQLParser.parseQuery2(Bigdata2ASTSPARQLParser.java:404)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:741)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:695)\\n\\tat com.bigdata.rdf.task.ApiTaskForIndexManager.call(ApiTaskForIndexManager.java:68)\\n\\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\t... 1 more\\nCaused by: com.bigdata.rdf.sail.sparql.ast.TokenMgrError: Lexical error at line 3, column 39.  Encountered: \"\\\\\"\" (34), after : \"Snooki\"\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilderTokenManager.getNextToken(SyntaxTreeBuilderTokenManager.java:3994)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.jj_ntk(SyntaxTreeBuilder.java:9637)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.RDFLiteral(SyntaxTreeBuilder.java:7189)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphTerm(SyntaxTreeBuilder.java:3893)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.VarOrTermOrTRefP(SyntaxTreeBuilder.java:8714)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphNodePath(SyntaxTreeBuilder.java:3786)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.ObjectPath(SyntaxTreeBuilder.java:3467)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.ObjectListPath(SyntaxTreeBuilder.java:3044)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.PropertyListPath(SyntaxTreeBuilder.java:2992)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.TriplesSameSubjectPath(SyntaxTreeBuilder.java:2919)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.TriplesBlock(SyntaxTreeBuilder.java:2312)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.BasicGraphPattern(SyntaxTreeBuilder.java:2097)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphPattern(SyntaxTreeBuilder.java:2034)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GroupGraphPattern(SyntaxTreeBuilder.java:1969)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.WhereClause(SyntaxTreeBuilder.java:1013)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.SelectQuery(SyntaxTreeBuilder.java:377)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.Query(SyntaxTreeBuilder.java:328)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.QueryContainer(SyntaxTreeBuilder.java:216)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.parseQuery(SyntaxTreeBuilder.java:32)\\n\\tat com.bigdata.rdf.sail.sparql.Bigdata2ASTSPARQLParser.parseQuery2(Bigdata2ASTSPARQLParser.java:336)\\n\\t... 7 more\\n'\n",
      "Error fetching QID: QueryBadFormed: A bad request has been sent to the endpoint: probably the SPARQL query is badly formed. \n",
      "\n",
      "Response:\n",
      "b'SPARQL-QUERY: queryStr=\\n    SELECT ?item WHERE {\\n      ?item rdfs:label \"Michael \"\"@en.\\n    } LIMIT 1\\n    \\njava.util.concurrent.ExecutionException: org.openrdf.query.MalformedQueryException: Lexical error at line 3, column 39.  Encountered: \"\\\\n\" (10), after : \"\\\\\"@en.\"\\n\\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:206)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doGet(QueryServlet.java:290)\\n\\tat com.bigdata.rdf.sail.webapp.RESTServlet.doGet(RESTServlet.java:240)\\n\\tat com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doGet(MultiTenancyServlet.java:273)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFilter.doFilter(ThrottlingFilter.java:322)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.SystemOverloadFilter.doFilter(SystemOverloadFilter.java:84)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat ch.qos.logback.classic.helpers.MDCInsertingServletFilter.doFilter(MDCInsertingServletFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.QueryEventSenderFilter.doFilter(QueryEventSenderFilter.java:125)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.ClientIPFilter.doFilter(ClientIPFilter.java:43)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.JWTIdentityFilter.doFilter(JWTIdentityFilter.java:66)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RealAgentFilter.doFilter(RealAgentFilter.java:33)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RequestConcurrencyFilter.doFilter(RequestConcurrencyFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\\n\\tat java.lang.Thread.run(Thread.java:750)\\nCaused by: org.openrdf.query.MalformedQueryException: Lexical error at line 3, column 39.  Encountered: \"\\\\n\" (10), after : \"\\\\\"@en.\"\\n\\tat com.bigdata.rdf.sail.sparql.Bigdata2ASTSPARQLParser.parseQuery2(Bigdata2ASTSPARQLParser.java:404)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:741)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:695)\\n\\tat com.bigdata.rdf.task.ApiTaskForIndexManager.call(ApiTaskForIndexManager.java:68)\\n\\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\t... 1 more\\nCaused by: com.bigdata.rdf.sail.sparql.ast.TokenMgrError: Lexical error at line 3, column 39.  Encountered: \"\\\\n\" (10), after : \"\\\\\"@en.\"\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilderTokenManager.getNextToken(SyntaxTreeBuilderTokenManager.java:3994)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.jj_ntk(SyntaxTreeBuilder.java:9637)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.RDFLiteral(SyntaxTreeBuilder.java:7189)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphTerm(SyntaxTreeBuilder.java:3893)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.VarOrTermOrTRefP(SyntaxTreeBuilder.java:8714)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphNodePath(SyntaxTreeBuilder.java:3786)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.ObjectPath(SyntaxTreeBuilder.java:3467)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.ObjectListPath(SyntaxTreeBuilder.java:3044)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.PropertyListPath(SyntaxTreeBuilder.java:2992)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.TriplesSameSubjectPath(SyntaxTreeBuilder.java:2919)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.TriplesBlock(SyntaxTreeBuilder.java:2312)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.BasicGraphPattern(SyntaxTreeBuilder.java:2097)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphPattern(SyntaxTreeBuilder.java:2034)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GroupGraphPattern(SyntaxTreeBuilder.java:1969)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.WhereClause(SyntaxTreeBuilder.java:1013)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.SelectQuery(SyntaxTreeBuilder.java:377)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.Query(SyntaxTreeBuilder.java:328)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.QueryContainer(SyntaxTreeBuilder.java:216)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.parseQuery(SyntaxTreeBuilder.java:32)\\n\\tat com.bigdata.rdf.sail.sparql.Bigdata2ASTSPARQLParser.parseQuery2(Bigdata2ASTSPARQLParser.java:336)\\n\\t... 7 more\\n'\n",
      "Error fetching QID: QueryBadFormed: A bad request has been sent to the endpoint: probably the SPARQL query is badly formed. \n",
      "\n",
      "Response:\n",
      "b'SPARQL-QUERY: queryStr=\\n    SELECT ?item WHERE {\\n      ?item rdfs:label \"Ex-MTV \"\"@en.\\n    } LIMIT 1\\n    \\njava.util.concurrent.ExecutionException: org.openrdf.query.MalformedQueryException: Lexical error at line 3, column 38.  Encountered: \"\\\\n\" (10), after : \"\\\\\"@en.\"\\n\\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:206)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doGet(QueryServlet.java:290)\\n\\tat com.bigdata.rdf.sail.webapp.RESTServlet.doGet(RESTServlet.java:240)\\n\\tat com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doGet(MultiTenancyServlet.java:273)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFilter.doFilter(ThrottlingFilter.java:322)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.SystemOverloadFilter.doFilter(SystemOverloadFilter.java:84)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat ch.qos.logback.classic.helpers.MDCInsertingServletFilter.doFilter(MDCInsertingServletFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.QueryEventSenderFilter.doFilter(QueryEventSenderFilter.java:125)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.ClientIPFilter.doFilter(ClientIPFilter.java:43)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.JWTIdentityFilter.doFilter(JWTIdentityFilter.java:66)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RealAgentFilter.doFilter(RealAgentFilter.java:33)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RequestConcurrencyFilter.doFilter(RequestConcurrencyFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\\n\\tat java.lang.Thread.run(Thread.java:750)\\nCaused by: org.openrdf.query.MalformedQueryException: Lexical error at line 3, column 38.  Encountered: \"\\\\n\" (10), after : \"\\\\\"@en.\"\\n\\tat com.bigdata.rdf.sail.sparql.Bigdata2ASTSPARQLParser.parseQuery2(Bigdata2ASTSPARQLParser.java:404)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:741)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:695)\\n\\tat com.bigdata.rdf.task.ApiTaskForIndexManager.call(ApiTaskForIndexManager.java:68)\\n\\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\t... 1 more\\nCaused by: com.bigdata.rdf.sail.sparql.ast.TokenMgrError: Lexical error at line 3, column 38.  Encountered: \"\\\\n\" (10), after : \"\\\\\"@en.\"\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilderTokenManager.getNextToken(SyntaxTreeBuilderTokenManager.java:3994)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.jj_ntk(SyntaxTreeBuilder.java:9637)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.RDFLiteral(SyntaxTreeBuilder.java:7189)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphTerm(SyntaxTreeBuilder.java:3893)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.VarOrTermOrTRefP(SyntaxTreeBuilder.java:8714)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphNodePath(SyntaxTreeBuilder.java:3786)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.ObjectPath(SyntaxTreeBuilder.java:3467)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.ObjectListPath(SyntaxTreeBuilder.java:3044)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.PropertyListPath(SyntaxTreeBuilder.java:2992)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.TriplesSameSubjectPath(SyntaxTreeBuilder.java:2919)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.TriplesBlock(SyntaxTreeBuilder.java:2312)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.BasicGraphPattern(SyntaxTreeBuilder.java:2097)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphPattern(SyntaxTreeBuilder.java:2034)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GroupGraphPattern(SyntaxTreeBuilder.java:1969)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.WhereClause(SyntaxTreeBuilder.java:1013)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.SelectQuery(SyntaxTreeBuilder.java:377)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.Query(SyntaxTreeBuilder.java:328)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.QueryContainer(SyntaxTreeBuilder.java:216)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.parseQuery(SyntaxTreeBuilder.java:32)\\n\\tat com.bigdata.rdf.sail.sparql.Bigdata2ASTSPARQLParser.parseQuery2(Bigdata2ASTSPARQLParser.java:336)\\n\\t... 7 more\\n'\n",
      "Error fetching QID: QueryBadFormed: A bad request has been sent to the endpoint: probably the SPARQL query is badly formed. \n",
      "\n",
      "Response:\n",
      "b'SPARQL-QUERY: queryStr=\\n    SELECT ?item WHERE {\\n      ?item rdfs:label \"Eldrick Tont \"Tiger\" Woods\"@en.\\n    } LIMIT 1\\n    \\njava.util.concurrent.ExecutionException: org.openrdf.query.MalformedQueryException: Lexical error at line 3, column 44.  Encountered: \"\\\\\"\" (34), after : \"Tiger\"\\n\\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:206)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doGet(QueryServlet.java:290)\\n\\tat com.bigdata.rdf.sail.webapp.RESTServlet.doGet(RESTServlet.java:240)\\n\\tat com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doGet(MultiTenancyServlet.java:273)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFilter.doFilter(ThrottlingFilter.java:322)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.SystemOverloadFilter.doFilter(SystemOverloadFilter.java:84)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat ch.qos.logback.classic.helpers.MDCInsertingServletFilter.doFilter(MDCInsertingServletFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.QueryEventSenderFilter.doFilter(QueryEventSenderFilter.java:125)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.ClientIPFilter.doFilter(ClientIPFilter.java:43)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.JWTIdentityFilter.doFilter(JWTIdentityFilter.java:66)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RealAgentFilter.doFilter(RealAgentFilter.java:33)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RequestConcurrencyFilter.doFilter(RequestConcurrencyFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\\n\\tat java.lang.Thread.run(Thread.java:750)\\nCaused by: org.openrdf.query.MalformedQueryException: Lexical error at line 3, column 44.  Encountered: \"\\\\\"\" (34), after : \"Tiger\"\\n\\tat com.bigdata.rdf.sail.sparql.Bigdata2ASTSPARQLParser.parseQuery2(Bigdata2ASTSPARQLParser.java:404)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:741)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:695)\\n\\tat com.bigdata.rdf.task.ApiTaskForIndexManager.call(ApiTaskForIndexManager.java:68)\\n\\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\t... 1 more\\nCaused by: com.bigdata.rdf.sail.sparql.ast.TokenMgrError: Lexical error at line 3, column 44.  Encountered: \"\\\\\"\" (34), after : \"Tiger\"\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilderTokenManager.getNextToken(SyntaxTreeBuilderTokenManager.java:3994)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.jj_ntk(SyntaxTreeBuilder.java:9637)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.RDFLiteral(SyntaxTreeBuilder.java:7189)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphTerm(SyntaxTreeBuilder.java:3893)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.VarOrTermOrTRefP(SyntaxTreeBuilder.java:8714)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphNodePath(SyntaxTreeBuilder.java:3786)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.ObjectPath(SyntaxTreeBuilder.java:3467)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.ObjectListPath(SyntaxTreeBuilder.java:3044)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.PropertyListPath(SyntaxTreeBuilder.java:2992)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.TriplesSameSubjectPath(SyntaxTreeBuilder.java:2919)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.TriplesBlock(SyntaxTreeBuilder.java:2312)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.BasicGraphPattern(SyntaxTreeBuilder.java:2097)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphPattern(SyntaxTreeBuilder.java:2034)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GroupGraphPattern(SyntaxTreeBuilder.java:1969)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.WhereClause(SyntaxTreeBuilder.java:1013)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.SelectQuery(SyntaxTreeBuilder.java:377)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.Query(SyntaxTreeBuilder.java:328)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.QueryContainer(SyntaxTreeBuilder.java:216)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.parseQuery(SyntaxTreeBuilder.java:32)\\n\\tat com.bigdata.rdf.sail.sparql.Bigdata2ASTSPARQLParser.parseQuery2(Bigdata2ASTSPARQLParser.java:336)\\n\\t... 7 more\\n'\n",
      "  [Train] Batch 200/212 â€“ Loss: 0.0393\n",
      "  [Train] Batch 212/212 â€“ Loss: 0.0327\n",
      "  Avg Train Loss: 0.0399\n",
      "Error fetching QID: QueryBadFormed: A bad request has been sent to the endpoint: probably the SPARQL query is badly formed. \n",
      "\n",
      "Response:\n",
      "b'SPARQL-QUERY: queryStr=\\n    SELECT ?item WHERE {\\n      ?item rdfs:label \"Mike \"Sugar Bear\" Thompson\"@en.\\n    } LIMIT 1\\n    \\njava.util.concurrent.ExecutionException: org.openrdf.query.MalformedQueryException: Lexical error at line 3, column 36.  Encountered: \" \" (32), after : \"Sugar\"\\n\\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:206)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doGet(QueryServlet.java:290)\\n\\tat com.bigdata.rdf.sail.webapp.RESTServlet.doGet(RESTServlet.java:240)\\n\\tat com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doGet(MultiTenancyServlet.java:273)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFilter.doFilter(ThrottlingFilter.java:322)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.SystemOverloadFilter.doFilter(SystemOverloadFilter.java:84)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat ch.qos.logback.classic.helpers.MDCInsertingServletFilter.doFilter(MDCInsertingServletFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.QueryEventSenderFilter.doFilter(QueryEventSenderFilter.java:125)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.ClientIPFilter.doFilter(ClientIPFilter.java:43)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.JWTIdentityFilter.doFilter(JWTIdentityFilter.java:66)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RealAgentFilter.doFilter(RealAgentFilter.java:33)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RequestConcurrencyFilter.doFilter(RequestConcurrencyFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\\n\\tat java.lang.Thread.run(Thread.java:750)\\nCaused by: org.openrdf.query.MalformedQueryException: Lexical error at line 3, column 36.  Encountered: \" \" (32), after : \"Sugar\"\\n\\tat com.bigdata.rdf.sail.sparql.Bigdata2ASTSPARQLParser.parseQuery2(Bigdata2ASTSPARQLParser.java:404)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:741)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:695)\\n\\tat com.bigdata.rdf.task.ApiTaskForIndexManager.call(ApiTaskForIndexManager.java:68)\\n\\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\t... 1 more\\nCaused by: com.bigdata.rdf.sail.sparql.ast.TokenMgrError: Lexical error at line 3, column 36.  Encountered: \" \" (32), after : \"Sugar\"\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilderTokenManager.getNextToken(SyntaxTreeBuilderTokenManager.java:3994)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.jj_ntk(SyntaxTreeBuilder.java:9637)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.RDFLiteral(SyntaxTreeBuilder.java:7189)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphTerm(SyntaxTreeBuilder.java:3893)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.VarOrTermOrTRefP(SyntaxTreeBuilder.java:8714)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphNodePath(SyntaxTreeBuilder.java:3786)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.ObjectPath(SyntaxTreeBuilder.java:3467)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.ObjectListPath(SyntaxTreeBuilder.java:3044)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.PropertyListPath(SyntaxTreeBuilder.java:2992)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.TriplesSameSubjectPath(SyntaxTreeBuilder.java:2919)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.TriplesBlock(SyntaxTreeBuilder.java:2312)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.BasicGraphPattern(SyntaxTreeBuilder.java:2097)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphPattern(SyntaxTreeBuilder.java:2034)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GroupGraphPattern(SyntaxTreeBuilder.java:1969)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.WhereClause(SyntaxTreeBuilder.java:1013)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.SelectQuery(SyntaxTreeBuilder.java:377)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.Query(SyntaxTreeBuilder.java:328)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.QueryContainer(SyntaxTreeBuilder.java:216)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.parseQuery(SyntaxTreeBuilder.java:32)\\n\\tat com.bigdata.rdf.sail.sparql.Bigdata2ASTSPARQLParser.parseQuery2(Bigdata2ASTSPARQLParser.java:336)\\n\\t... 7 more\\n'\n",
      "Error fetching QID: QueryBadFormed: A bad request has been sent to the endpoint: probably the SPARQL query is badly formed. \n",
      "\n",
      "Response:\n",
      "b'SPARQL-QUERY: queryStr=\\n    SELECT ?item WHERE {\\n      ?item rdfs:label \"The \"Jersey Shore\"@en.\\n    } LIMIT 1\\n    \\njava.util.concurrent.ExecutionException: org.openrdf.query.MalformedQueryException: Lexical error at line 3, column 36.  Encountered: \" \" (32), after : \"Jersey\"\\n\\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:206)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doGet(QueryServlet.java:290)\\n\\tat com.bigdata.rdf.sail.webapp.RESTServlet.doGet(RESTServlet.java:240)\\n\\tat com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doGet(MultiTenancyServlet.java:273)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFilter.doFilter(ThrottlingFilter.java:322)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.SystemOverloadFilter.doFilter(SystemOverloadFilter.java:84)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat ch.qos.logback.classic.helpers.MDCInsertingServletFilter.doFilter(MDCInsertingServletFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.QueryEventSenderFilter.doFilter(QueryEventSenderFilter.java:125)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.ClientIPFilter.doFilter(ClientIPFilter.java:43)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.JWTIdentityFilter.doFilter(JWTIdentityFilter.java:66)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RealAgentFilter.doFilter(RealAgentFilter.java:33)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RequestConcurrencyFilter.doFilter(RequestConcurrencyFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\\n\\tat java.lang.Thread.run(Thread.java:750)\\nCaused by: org.openrdf.query.MalformedQueryException: Lexical error at line 3, column 36.  Encountered: \" \" (32), after : \"Jersey\"\\n\\tat com.bigdata.rdf.sail.sparql.Bigdata2ASTSPARQLParser.parseQuery2(Bigdata2ASTSPARQLParser.java:404)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:741)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:695)\\n\\tat com.bigdata.rdf.task.ApiTaskForIndexManager.call(ApiTaskForIndexManager.java:68)\\n\\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\t... 1 more\\nCaused by: com.bigdata.rdf.sail.sparql.ast.TokenMgrError: Lexical error at line 3, column 36.  Encountered: \" \" (32), after : \"Jersey\"\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilderTokenManager.getNextToken(SyntaxTreeBuilderTokenManager.java:3994)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.jj_ntk(SyntaxTreeBuilder.java:9637)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.RDFLiteral(SyntaxTreeBuilder.java:7189)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphTerm(SyntaxTreeBuilder.java:3893)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.VarOrTermOrTRefP(SyntaxTreeBuilder.java:8714)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphNodePath(SyntaxTreeBuilder.java:3786)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.ObjectPath(SyntaxTreeBuilder.java:3467)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.ObjectListPath(SyntaxTreeBuilder.java:3044)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.PropertyListPath(SyntaxTreeBuilder.java:2992)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.TriplesSameSubjectPath(SyntaxTreeBuilder.java:2919)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.TriplesBlock(SyntaxTreeBuilder.java:2312)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.BasicGraphPattern(SyntaxTreeBuilder.java:2097)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphPattern(SyntaxTreeBuilder.java:2034)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GroupGraphPattern(SyntaxTreeBuilder.java:1969)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.WhereClause(SyntaxTreeBuilder.java:1013)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.SelectQuery(SyntaxTreeBuilder.java:377)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.Query(SyntaxTreeBuilder.java:328)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.QueryContainer(SyntaxTreeBuilder.java:216)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.parseQuery(SyntaxTreeBuilder.java:32)\\n\\tat com.bigdata.rdf.sail.sparql.Bigdata2ASTSPARQLParser.parseQuery2(Bigdata2ASTSPARQLParser.java:336)\\n\\t... 7 more\\n'\n",
      "Error fetching QID: QueryBadFormed: A bad request has been sent to the endpoint: probably the SPARQL query is badly formed. \n",
      "\n",
      "Response:\n",
      "b'SPARQL-QUERY: queryStr=\\n    SELECT ?item WHERE {\\n      ?item rdfs:label \"Kimberleigh Marie \"\"@en.\\n    } LIMIT 1\\n    \\njava.util.concurrent.ExecutionException: org.openrdf.query.MalformedQueryException: Lexical error at line 3, column 49.  Encountered: \"\\\\n\" (10), after : \"\\\\\"@en.\"\\n\\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\\n\\tat java.util.concurrent.FutureTask.get(FutureTask.java:206)\\n\\tat com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet.doGet(QueryServlet.java:290)\\n\\tat com.bigdata.rdf.sail.webapp.RESTServlet.doGet(RESTServlet.java:240)\\n\\tat com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doGet(MultiTenancyServlet.java:273)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\\n\\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\\n\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFilter.doFilter(ThrottlingFilter.java:322)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.throttling.SystemOverloadFilter.doFilter(SystemOverloadFilter.java:84)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat ch.qos.logback.classic.helpers.MDCInsertingServletFilter.doFilter(MDCInsertingServletFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.QueryEventSenderFilter.doFilter(QueryEventSenderFilter.java:125)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.ClientIPFilter.doFilter(ClientIPFilter.java:43)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.JWTIdentityFilter.doFilter(JWTIdentityFilter.java:66)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RealAgentFilter.doFilter(RealAgentFilter.java:33)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\\n\\tat org.wikidata.query.rdf.blazegraph.filters.RequestConcurrencyFilter.doFilter(RequestConcurrencyFilter.java:50)\\n\\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)\\n\\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1340)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\\n\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\\n\\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\\n\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1242)\\n\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\\n\\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)\\n\\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\\n\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\\n\\tat org.eclipse.jetty.server.Server.handle(Server.java:503)\\n\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)\\n\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\\n\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\\n\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\\n\\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\\n\\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\\n\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\\n\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\\n\\tat java.lang.Thread.run(Thread.java:750)\\nCaused by: org.openrdf.query.MalformedQueryException: Lexical error at line 3, column 49.  Encountered: \"\\\\n\" (10), after : \"\\\\\"@en.\"\\n\\tat com.bigdata.rdf.sail.sparql.Bigdata2ASTSPARQLParser.parseQuery2(Bigdata2ASTSPARQLParser.java:404)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:741)\\n\\tat com.bigdata.rdf.sail.webapp.QueryServlet$SparqlQueryTask.call(QueryServlet.java:695)\\n\\tat com.bigdata.rdf.task.ApiTaskForIndexManager.call(ApiTaskForIndexManager.java:68)\\n\\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\t... 1 more\\nCaused by: com.bigdata.rdf.sail.sparql.ast.TokenMgrError: Lexical error at line 3, column 49.  Encountered: \"\\\\n\" (10), after : \"\\\\\"@en.\"\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilderTokenManager.getNextToken(SyntaxTreeBuilderTokenManager.java:3994)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.jj_ntk(SyntaxTreeBuilder.java:9637)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.RDFLiteral(SyntaxTreeBuilder.java:7189)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphTerm(SyntaxTreeBuilder.java:3893)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.VarOrTermOrTRefP(SyntaxTreeBuilder.java:8714)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphNodePath(SyntaxTreeBuilder.java:3786)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.ObjectPath(SyntaxTreeBuilder.java:3467)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.ObjectListPath(SyntaxTreeBuilder.java:3044)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.PropertyListPath(SyntaxTreeBuilder.java:2992)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.TriplesSameSubjectPath(SyntaxTreeBuilder.java:2919)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.TriplesBlock(SyntaxTreeBuilder.java:2312)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.BasicGraphPattern(SyntaxTreeBuilder.java:2097)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GraphPattern(SyntaxTreeBuilder.java:2034)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.GroupGraphPattern(SyntaxTreeBuilder.java:1969)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.WhereClause(SyntaxTreeBuilder.java:1013)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.SelectQuery(SyntaxTreeBuilder.java:377)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.Query(SyntaxTreeBuilder.java:328)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.QueryContainer(SyntaxTreeBuilder.java:216)\\n\\tat com.bigdata.rdf.sail.sparql.ast.SyntaxTreeBuilder.parseQuery(SyntaxTreeBuilder.java:32)\\n\\tat com.bigdata.rdf.sail.sparql.Bigdata2ASTSPARQLParser.parseQuery2(Bigdata2ASTSPARQLParser.java:336)\\n\\t... 7 more\\n'\n",
      "\n",
      "â€” Epoch 2/10 â€”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinpatel/venv_310/lib/python3.10/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Train] Batch 100/212 â€“ Loss: 0.0180\n",
      "  [Train] Batch 200/212 â€“ Loss: 0.0286\n",
      "  [Train] Batch 212/212 â€“ Loss: 0.0285\n",
      "  Avg Train Loss: 0.0287\n",
      "\n",
      "â€” Epoch 3/10 â€”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinpatel/venv_310/lib/python3.10/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Train] Batch 100/212 â€“ Loss: 0.0295\n",
      "  [Train] Batch 200/212 â€“ Loss: 0.0196\n",
      "  [Train] Batch 212/212 â€“ Loss: 0.0266\n",
      "  Avg Train Loss: 0.0208\n",
      "\n",
      "â€” Epoch 4/10 â€”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinpatel/venv_310/lib/python3.10/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Train] Batch 100/212 â€“ Loss: 0.0049\n",
      "  [Train] Batch 200/212 â€“ Loss: 0.0136\n",
      "  [Train] Batch 212/212 â€“ Loss: 0.0051\n",
      "  Avg Train Loss: 0.0119\n",
      "    â†’ No improvement for 1/3 epochs\n",
      "\n",
      "â€” Epoch 5/10 â€”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinpatel/venv_310/lib/python3.10/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Train] Batch 100/212 â€“ Loss: 0.0080\n",
      "  [Train] Batch 200/212 â€“ Loss: 0.0148\n",
      "  [Train] Batch 212/212 â€“ Loss: 0.0010\n",
      "  Avg Train Loss: 0.0062\n",
      "    â†’ No improvement for 2/3 epochs\n",
      "\n",
      "â€” Epoch 6/10 â€”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinpatel/venv_310/lib/python3.10/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Train] Batch 100/212 â€“ Loss: 0.0020\n",
      "  [Train] Batch 200/212 â€“ Loss: 0.0007\n",
      "  [Train] Batch 212/212 â€“ Loss: 0.0102\n",
      "  Avg Train Loss: 0.0039\n",
      "    â†’ No improvement for 3/3 epochs\n",
      "    Early stopping at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinpatel/venv_310/lib/python3.10/site-packages/PIL/Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Final PolitiFact Results:\n",
      "âœ… Accuracy:             0.8681\n",
      "ðŸŽ¯ F1 Score:             0.8894\n",
      "ðŸ” Precision:            0.9345\n",
      "ðŸ“¥ Recall:               0.8788\n",
      "\n",
      "ðŸ“– Avg Text Confidence:  0.5630\n",
      "ðŸ–¼ï¸ Avg Image Confidence: 0.5058\n",
      "ðŸ“š Avg KG Confidence:    0.4931\n",
      "\n",
      "ðŸ“Š Final GossipCop Results:\n",
      "âœ… Accuracy:             0.8851\n",
      "ðŸŽ¯ F1 Score:             0.9297\n",
      "ðŸ” Precision:            0.9265\n",
      "ðŸ“¥ Recall:               0.9592\n",
      "\n",
      "ðŸ“– Avg Text Confidence:  0.4319\n",
      "ðŸ–¼ï¸ Avg Image Confidence: 0.4414\n",
      "ðŸ“š Avg KG Confidence:    0.4933\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# â”€â”€â”€ Focal Loss Definition â”€â”€â”€\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=0.25, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        p_t = torch.exp(-bce)\n",
    "        loss = self.alpha * (1 - p_t) ** self.gamma * bce\n",
    "        return loss.mean() if self.reduction == 'mean' else loss.sum()\n",
    "\n",
    "# â”€â”€â”€ Training + Eval on Both Splits with Focal Loss + Threshold Tuning â”€â”€â”€\n",
    "epochs, patience = 10, 3\n",
    "results = {}\n",
    "\n",
    "for name, tr_loader, te_loader in [\n",
    "    (\"PolitiFact\", pf_train_loader, pf_test_loader),\n",
    "    (\"GossipCop\",  gc_train_loader, gc_test_loader),\n",
    "]:\n",
    "    print(f\"\\n=== Running on {name} split ===\")\n",
    "\n",
    "    # 1) Re-init model, optimizer, scheduler, loss\n",
    "    classifier = NewsClassifier()\n",
    "    optimizer  = AdamW([\n",
    "        {\"params\": roberta.encoder.layer[-4:].parameters(),           \"lr\": 5e-6, \"weight_decay\": 1e-4},\n",
    "        {\"params\": roberta.pooler.parameters(),                        \"lr\": 5e-6, \"weight_decay\": 1e-4},\n",
    "        {\"params\": clip_model.vision_model.encoder.layers[-4:].parameters(), \n",
    "         \"lr\": 5e-6, \"weight_decay\": 1e-4},\n",
    "        {\"params\": clip_model.visual_projection.parameters(),          \"lr\": 5e-6, \"weight_decay\": 1e-4},\n",
    "        {\"params\": classifier.parameters(),                           \"lr\": 3e-4, \"weight_decay\": 1e-3},\n",
    "    ])\n",
    "    criterion = FocalLoss(gamma=2.0, alpha=0.25)\n",
    "    total_steps = epochs * len(tr_loader)\n",
    "    scheduler   = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_val_acc, no_improve = 0.0, 0\n",
    "    best_thr_for_split = 0.5\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\nâ€” Epoch {epoch}/{epochs} â€”\")\n",
    "        # â€”â€”â€” Training â€”â€”â€”\n",
    "        classifier.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (x, y) in enumerate(tr_loader, start=1):\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = classifier(x)                  # unpack tuple here\n",
    "            loss      = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 0 or i == len(tr_loader):\n",
    "                print(f\"  [Train] Batch {i}/{len(tr_loader)} â€“ Loss: {loss:.4f}\")\n",
    "        print(f\"  Avg Train Loss: {running_loss/len(tr_loader):.4f}\")\n",
    "\n",
    "        # â€”â€”â€” Validation & Threshold Sweep â€”â€”â€”\n",
    "        classifier.eval()\n",
    "        all_probs, all_y = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in te_loader:\n",
    "                logits, _ = classifier(x)             # unpack tuple here\n",
    "                probs      = torch.sigmoid(logits).view(-1).tolist()\n",
    "                labels     = y.view(-1).tolist()\n",
    "                all_probs.extend(probs)\n",
    "                all_y.extend(labels)\n",
    "\n",
    "        # compute precision/recall curve & pick threshold maxing F1\n",
    "        precisions, recalls, ths = precision_recall_curve(all_y, all_probs)\n",
    "        f1_scores = 2 * precisions * recalls / (precisions + recalls + 1e-8)\n",
    "        best_idx  = f1_scores.argmax()\n",
    "        best_thr  = ths[best_idx]\n",
    "        best_thr_for_split = best_thr\n",
    "\n",
    "        # compute early-stop metric (accuracy) at this threshold\n",
    "        val_preds = [1 if p > best_thr else 0 for p in all_probs]\n",
    "        val_acc   = accuracy_score(all_y, val_preds)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc, no_improve = val_acc, 0\n",
    "            torch.save(classifier.state_dict(), f\"best_{name}.pt\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            print(f\"    â†’ No improvement for {no_improve}/{patience} epochs\")\n",
    "            if no_improve >= patience:\n",
    "                print(f\"    Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    # â€”â€”â€” Final Eval @ chosen threshold + collect confidences â€”â€”â€”\n",
    "    classifier.load_state_dict(torch.load(f\"best_{name}.pt\"))\n",
    "    classifier.eval()\n",
    "\n",
    "    final_probs, final_y = [], []\n",
    "    text_confs, image_confs, kg_confs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in te_loader:\n",
    "            # 1) get model outputs\n",
    "            logit, confs = classifier(x)\n",
    "\n",
    "            # 2) probabilities and true labels\n",
    "            probs  = torch.sigmoid(logit).view(-1).tolist()\n",
    "            labels = y.view(-1).tolist()\n",
    "\n",
    "            final_probs.extend(probs)\n",
    "            final_y.extend(labels)\n",
    "\n",
    "            # 3) per-modality confidences\n",
    "            text_confs.extend(  confs['text_conf'].view(-1).tolist() )\n",
    "            image_confs.extend( confs['image_conf'].view(-1).tolist() )\n",
    "            kg_confs.extend(    confs['kg_conf'].view(-1).tolist() )\n",
    "\n",
    "    # apply your best threshold\n",
    "    final_preds = [1 if p > best_thr_for_split else 0 for p in final_probs]\n",
    "\n",
    "    # compute guarded averages for confidences\n",
    "    avg_txt = sum(text_confs)  / len(text_confs)  if text_confs  else 0.0\n",
    "    avg_img = sum(image_confs) / len(image_confs) if image_confs else 0.0\n",
    "    avg_kg  = sum(kg_confs)    / len(kg_confs)    if kg_confs    else 0.0\n",
    "\n",
    "    # store metrics + these averages\n",
    "    results[name] = {\n",
    "        \"accuracy\":       accuracy_score(final_y, final_preds),\n",
    "        \"f1\":             f1_score(final_y, final_preds),\n",
    "        \"precision\":      precision_score(final_y, final_preds),\n",
    "        \"recall\":         recall_score(final_y, final_preds),\n",
    "        \"threshold\":      best_thr_for_split,\n",
    "        \"avg_text_conf\":  avg_txt,\n",
    "        \"avg_image_conf\": avg_img,\n",
    "        \"avg_kg_conf\":    avg_kg,\n",
    "    }\n",
    "    \n",
    "\n",
    "for name, m in results.items():\n",
    "        print(f\"\\nðŸ“Š Final {name} Results:\")\n",
    "        print(f\"âœ… Accuracy:  {m['accuracy']:.4f}\")\n",
    "        print(f\"ðŸŽ¯ F1 Score:  {m['f1']:.4f}\")\n",
    "        print(f\"ðŸ” Precision: {m['precision']:.4f}\")\n",
    "        print(f\"ðŸ“¥ Recall:    {m['recall']:.4f}\")\n",
    "        print()\n",
    "        print(f\"ðŸ“Š Final {name} Results:\")\n",
    "        print(f\"ðŸ“– Avg Text Confidence:  {m['avg_text_conf']:.4f}\")\n",
    "        print(f\"ðŸ–¼ï¸  Avg Image Confidence: {m['avg_image_conf']:.4f}\")\n",
    "        print(f\"ðŸ“š Avg KG Confidence:    {m['avg_kg_conf']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
